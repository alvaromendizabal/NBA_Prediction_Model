{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = {\n",
    "        \"Atlanta Hawks\": 1610612737,\n",
    "        \"Boston Celtics\": 1610612738,\n",
    "        \"Brooklyn Nets\": 1610612751,\n",
    "        \"Charlotte Bobcats\": 1610612766,\n",
    "        \"Charlotte Hornets\": 1610612766,\n",
    "        \"Chicago Bulls\": 1610612741,\n",
    "        \"Cleveland Cavaliers\": 1610612739,\n",
    "        \"Dallas Mavericks\": 1610612742,\n",
    "        \"Denver Nuggets\": 1610612743,\n",
    "        \"Detroit Pistons\": 1610612765,\n",
    "        \"Golden State Warriors\": 1610612744,\n",
    "        \"Houston Rockets\": 1610612745,\n",
    "        \"Indiana Pacers\": 1610612754,\n",
    "        \"LA Clippers\": 1610612746,\n",
    "        \"Los Angeles Clippers\": 1610612746,\n",
    "        \"Los Angeles Lakers\": 1610612747,\n",
    "        \"Memphis Grizzlies\": 1610612763,\n",
    "        \"Miami Heat\": 1610612748,\n",
    "        \"Milwaukee Bucks\": 1610612749,\n",
    "        \"Minnesota Timberwolves\": 1610612750,\n",
    "        \"New Jersey Nets\": 1610612751,\n",
    "        \"New Orleans Hornets\": 1610612740,\n",
    "        \"New Orleans Pelicans\": 1610612740,\n",
    "        \"New York Knicks\": 1610612752,\n",
    "        \"Oklahoma City Thunder\": 1610612760,\n",
    "        \"Orlando Magic\": 1610612753,\n",
    "        \"Philadelphia 76ers\": 1610612755,\n",
    "        \"Phoenix Suns\": 1610612756,\n",
    "        \"Portland Trail Blazers\": 1610612757,\n",
    "        \"Sacramento Kings\": 1610612758,\n",
    "        \"San Antonio Spurs\": 1610612759,\n",
    "        \"Toronto Raptors\": 1610612761,\n",
    "        \"Utah Jazz\": 1610612762,\n",
    "        \"Washington Wizards\": 1610612764,\n",
    "    }\n",
    "\n",
    "available_stats = {'W_PCT': 'Base',\n",
    "                   'FG_PCT': 'Base',\n",
    "                   'FG3_PCT': 'Base',\n",
    "                   'FT_PCT': 'Base',\n",
    "                   'REB': 'Base',\n",
    "                   'AST': 'Base',\n",
    "                   'TOV': 'Base',\n",
    "                   'STL': 'Base',\n",
    "                   'BLK': 'Base',\n",
    "                   'PLUS_MINUS': 'Base',\n",
    "                   'OFF_RATING': 'Advanced',\n",
    "                   'DEF_RATING': 'Advanced',\n",
    "                   'TS_PCT': 'Advanced'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W_PCT': 0.646,\n",
       " 'FG_PCT': 0.469,\n",
       " 'FG3_PCT': 0.364,\n",
       " 'FT_PCT': 0.769,\n",
       " 'REB': 45.9,\n",
       " 'AST': 27.4,\n",
       " 'TOV': 15.0,\n",
       " 'STL': 8.9,\n",
       " 'BLK': 4.6,\n",
       " 'PLUS_MINUS': 5.6,\n",
       " 'OFF_RATING': 112.1,\n",
       " 'DEF_RATING': 106.6,\n",
       " 'TS_PCT': 0.582}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_stats.py gets the team data for the model\n",
    "\n",
    "# from team_names import teams\n",
    "import nba_api\n",
    "from nba_api.stats.endpoints import teamdashboardbygeneralsplits, leaguedashteamstats\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def get_team_stats_dict(team, start_date, end_date, season='2021-22'):\n",
    "    \"\"\"\n",
    "    Returns the stats for the specified team in a dataframe, default year is 2021-22\n",
    "    :param team: Day of games scheduled in form 'mm/dd/yyyy'\n",
    "    :param season: Day of games scheduled in form 'mm/dd/yyyy'\n",
    "    :return: A dictionary of game matchups {home_team:[away_team]}\n",
    "    \"\"\"\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    \n",
    "    general_team_info = teamdashboardbygeneralsplits.TeamDashboardByGeneralSplits(team_id=teams[team],\n",
    "                                                                                  per_mode_detailed='Per100Possessions',\n",
    "                                                                                  season=season,\n",
    "                                                                                  date_from_nullable=start_date,\n",
    "                                                                                  date_to_nullable=end_date,\n",
    "                                                                                  timeout=120)\n",
    "    general_team_dict = general_team_info.get_normalized_dict()\n",
    "    general_team_dashboard = general_team_dict['OverallTeamDashboard'][0]\n",
    "\n",
    "\n",
    "    win_percentage = general_team_dashboard['W_PCT']\n",
    "    fg_percentage = general_team_dashboard['FG_PCT']\n",
    "    fg3_percentage = general_team_dashboard['FG3_PCT']\n",
    "    ft_percentage = general_team_dashboard['FT_PCT']\n",
    "    rebounds = general_team_dashboard['REB']\n",
    "    assists = general_team_dashboard['AST']\n",
    "    turnovers = general_team_dashboard['TOV']\n",
    "    steals = general_team_dashboard['STL']\n",
    "    blocks = general_team_dashboard['BLK']\n",
    "    plus_minus = general_team_dashboard['PLUS_MINUS']\n",
    "\n",
    "    advanced_team_info = teamdashboardbygeneralsplits.TeamDashboardByGeneralSplits(team_id=teams[team],\n",
    "                                                                                   measure_type_detailed_defense='Advanced',\n",
    "                                                                                   season=season,\n",
    "                                                                                   date_from_nullable=start_date,\n",
    "                                                                                   date_to_nullable=end_date,\n",
    "                                                                                   timeout=120)\n",
    "    advanced_team_dict = advanced_team_info.get_normalized_dict()\n",
    "    advanced_team_dashboard = advanced_team_dict['OverallTeamDashboard'][0]\n",
    "\n",
    "    offensive_rating = advanced_team_dashboard['OFF_RATING']\n",
    "    defensive_rating = advanced_team_dashboard['DEF_RATING']\n",
    "    true_shooting_percentage = advanced_team_dashboard['TS_PCT']\n",
    "\n",
    "    all_stats_dict = {\n",
    "        'W_PCT': win_percentage,\n",
    "        'FG_PCT': fg_percentage,\n",
    "        'FG3_PCT': fg3_percentage,\n",
    "        'FT_PCT': ft_percentage,\n",
    "        'REB': rebounds,\n",
    "        'AST': assists,\n",
    "        'TOV': turnovers,\n",
    "        'STL': steals,\n",
    "        'BLK': blocks,\n",
    "        'PLUS_MINUS': plus_minus,\n",
    "        'OFF_RATING': offensive_rating,\n",
    "        'DEF_RATING': defensive_rating,\n",
    "        'TS_PCT': true_shooting_percentage\n",
    "    }\n",
    "\n",
    "    return all_stats_dict\n",
    "\n",
    "\n",
    "get_team_stats_dict('Golden State Warriors', '10/19/2021', '04/10/2022', '2021-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'get_daily_matches' returns a dictionary of the games on a specified date\n",
      "{'New York Knicks': 'Philadelphia 76ers', 'Dallas Mavericks': 'Los Angeles Lakers', 'Boston Celtics': 'Milwaukee Bucks', 'Golden State Warriors': 'Memphis Grizzlies', 'Denver Nuggets': 'Phoenix Suns'}\n",
      "\n",
      "'get_match_results' returns the matchup plus the result\n",
      "[{'Los Angeles Lakers': 'Golden State Warriors', 'Milwaukee Bucks': 'Brooklyn Nets'}, ['L', 'W'], [114, 121, 127, 104], ['0022100002', '0022100001']]\n"
     ]
    }
   ],
   "source": [
    "# get_matches.py gets the daily matches for a specific date and the results of the games\n",
    "\n",
    "from nba_api.stats.endpoints import leaguegamelog, scoreboard, leaguestandings\n",
    "#from team_names import teams\n",
    "\n",
    "\n",
    "\n",
    "def get_match_results(date, season):\n",
    "    \"\"\"\n",
    "    Returns the matchup and result of the game\n",
    "\n",
    "    :param date: Day of games scheduled in form 'mm/dd/yyyy'\n",
    "    :param season: Season in form of 'yyyy-yy'\n",
    "    :return: [{Boston Celtics: Los Angeles Lakers}], ['W']\n",
    "    \"\"\"\n",
    "\n",
    "    game_log = leaguegamelog.LeagueGameLog(season=season, league_id='00', date_from_nullable=date,\n",
    "                                           date_to_nullable=date, season_type_all_star='Regular Season', timeout=120)\n",
    "    game_log_dict = game_log.get_normalized_dict()\n",
    "    list_of_teams = game_log_dict['LeagueGameLog']\n",
    "\n",
    "    daily_match = {}\n",
    "    win_loss = []\n",
    "    score = []\n",
    "    game_id = []\n",
    "\n",
    "    for i in range(0, len(list_of_teams), 2):\n",
    "\n",
    "        if '@' in list_of_teams[i]['MATCHUP']:\n",
    "\n",
    "            away_team = list_of_teams[i]['TEAM_NAME']\n",
    "            home_team = list_of_teams[i + 1]['TEAM_NAME']\n",
    "\n",
    "            win_loss.append(list_of_teams[i + 1]['WL'])\n",
    "\n",
    "            game_id.append(list_of_teams[i + 1]['GAME_ID'])\n",
    "\n",
    "            score.append(list_of_teams[i + 1]['PTS'])\n",
    "            score.append(list_of_teams[i]['PTS'])\n",
    "\n",
    "        else:\n",
    "            away_team = list_of_teams[i + 1]['TEAM_NAME']\n",
    "            home_team = list_of_teams[i]['TEAM_NAME']\n",
    "\n",
    "            win_loss.append(list_of_teams[i]['WL'])\n",
    "\n",
    "            game_id.append(list_of_teams[i]['GAME_ID'])\n",
    "\n",
    "            score.append(list_of_teams[i]['PTS'])\n",
    "            score.append(list_of_teams[i + 1]['PTS'])\n",
    "\n",
    "        daily_match.update({home_team: away_team})\n",
    "\n",
    "    match_results = [daily_match, win_loss, score, game_id]\n",
    "\n",
    "    return match_results\n",
    "\n",
    "\n",
    "def get_daily_matches(date):\n",
    "    \"\"\"\n",
    "    This method creates a dictionary of daily game matchups.\n",
    "\n",
    "    :param date: Day of games scheduled in form 'mm/dd/yyyy'\n",
    "    :return: A dictionary of game matchups {home_team:away_team}\n",
    "    \"\"\"\n",
    "\n",
    "    daily_match = scoreboard.Scoreboard(league_id='00', game_date=date, timeout=120)\n",
    "    daily_match_dict = daily_match.get_normalized_dict()\n",
    "    games = daily_match_dict['GameHeader']\n",
    "\n",
    "    match = {}\n",
    "\n",
    "    for game in games:\n",
    "\n",
    "        home_team_id = game['HOME_TEAM_ID']\n",
    "\n",
    "        for team, team_id in teams.items():\n",
    "            if team_id == home_team_id:\n",
    "                home_team = team\n",
    "\n",
    "        away_team_id = game['VISITOR_TEAM_ID']\n",
    "\n",
    "        for team, team_id in teams.items():\n",
    "            if team_id == away_team_id:\n",
    "                away_team = team\n",
    "\n",
    "        match.update({home_team: away_team})\n",
    "\n",
    "    return match\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(f\"\"\"'get_daily_matches' returns a dictionary of the games on a specified date\\n{get_daily_matches('12/25/22')}\\n\"\"\")\n",
    "    print(f\"\"\"'get_match_results' returns the matchup plus the result\\n{get_match_results('10/19/2021', '2021-22')}\"\"\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/22/2021\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 125>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 126\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(attempts):\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 112\u001b[0m         all_games \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_year\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2021\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_month\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_day\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m22\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_year\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2022\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_month\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_day\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mseason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2021-22\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseason_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m10/19/2021\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m         df \u001b[38;5;241m=\u001b[39m make_dataframe(all_games)\n\u001b[0;32m    116\u001b[0m         \u001b[38;5;28mprint\u001b[39m(df)\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mtraining_set\u001b[1;34m(start_year, start_month, start_day, end_year, end_month, end_day, season, season_start)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m#mean_std_dictionary = mean_std_dict(season_start, previous_day_formatted, season)\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m#mean_dict = mean_std_dictionary[0]\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m#std_dict = mean_std_dictionary[1]\u001b[39;00m\n\u001b[0;32m     82\u001b[0m current_day_games \u001b[38;5;241m=\u001b[39m get_match_results(current_date, season)\n\u001b[1;32m---> 83\u001b[0m current_day_games_with_stats \u001b[38;5;241m=\u001b[39m \u001b[43mto_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_day_games\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseason_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_day_formatted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseason\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m game \u001b[38;5;129;01min\u001b[39;00m current_day_games_with_stats:\n\u001b[0;32m     86\u001b[0m     game\u001b[38;5;241m.\u001b[39mappend(current_date)\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mto_dataframe\u001b[1;34m(daily_games, start_date, end_date, season)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m home_team, away_team \u001b[38;5;129;01min\u001b[39;00m daily_games[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitems():  \u001b[38;5;66;03m# loops through matchups\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     home_team_stats \u001b[38;5;241m=\u001b[39m get_team_stats_dict(home_team, start_date, end_date, season)\n\u001b[1;32m---> 22\u001b[0m     away_team_stats \u001b[38;5;241m=\u001b[39m \u001b[43mget_team_stats_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43maway_team\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseason\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     current_game \u001b[38;5;241m=\u001b[39m [home_team, away_team]\n\u001b[0;32m     26\u001b[0m     current_game\u001b[38;5;241m.\u001b[39mappend(game_id[game_number])\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mget_team_stats_dict\u001b[1;34m(team, start_date, end_date, season)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03mReturns the stats for the specified team in a dataframe, default year is 2021-22\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m:param team: Day of games scheduled in form 'mm/dd/yyyy'\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m:param season: Day of games scheduled in form 'mm/dd/yyyy'\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m:return: A dictionary of game matchups {home_team:[away_team]}\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     19\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m general_team_info \u001b[38;5;241m=\u001b[39m \u001b[43mteamdashboardbygeneralsplits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTeamDashboardByGeneralSplits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteam_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mteams\u001b[49m\u001b[43m[\u001b[49m\u001b[43mteam\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m                                                                              \u001b[49m\u001b[43mper_mode_detailed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPer100Possessions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                                                                              \u001b[49m\u001b[43mseason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseason\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m                                                                              \u001b[49m\u001b[43mdate_from_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m                                                                              \u001b[49m\u001b[43mdate_to_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m                                                                              \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m120\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m general_team_dict \u001b[38;5;241m=\u001b[39m general_team_info\u001b[38;5;241m.\u001b[39mget_normalized_dict()\n\u001b[0;32m     29\u001b[0m general_team_dashboard \u001b[38;5;241m=\u001b[39m general_team_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOverallTeamDashboard\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nba_api\\stats\\endpoints\\teamdashboardbygeneralsplits.py:74\u001b[0m, in \u001b[0;36mTeamDashboardByGeneralSplits.__init__\u001b[1;34m(self, team_id, last_n_games, measure_type_detailed_defense, month, opponent_team_id, pace_adjust, per_mode_detailed, period, plus_minus, rank, season, season_type_all_star, date_from_nullable, date_to_nullable, game_segment_nullable, league_id_nullable, location_nullable, outcome_nullable, po_round_nullable, season_segment_nullable, shot_clock_range_nullable, vs_conference_nullable, vs_division_nullable, proxy, headers, timeout, get_request)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTeamID\u001b[39m\u001b[38;5;124m'\u001b[39m: team_id,\n\u001b[0;32m     50\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLastNGames\u001b[39m\u001b[38;5;124m'\u001b[39m: last_n_games,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVsDivision\u001b[39m\u001b[38;5;124m'\u001b[39m: vs_division_nullable\n\u001b[0;32m     72\u001b[0m }\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_request:\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nba_api\\stats\\endpoints\\teamdashboardbygeneralsplits.py:77\u001b[0m, in \u001b[0;36mTeamDashboardByGeneralSplits.get_request\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_request\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnba_response \u001b[38;5;241m=\u001b[39m \u001b[43mNBAStatsHTTP\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_api_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_response()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\nba_api\\library\\http.py:130\u001b[0m, in \u001b[0;36mNBAHTTP.send_api_request\u001b[1;34m(self, endpoint, parameters, referer, proxy, headers, timeout, raise_exception_on_error)\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloading from file...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m contents:\n\u001b[1;32m--> 130\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     url \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39murl\n\u001b[0;32m    132\u001b[0m     status_code \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py:75\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget\u001b[39m\u001b[38;5;124m'\u001b[39m, url, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py:61\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py:529\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    524\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m: timeout,\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m'\u001b[39m: allow_redirects,\n\u001b[0;32m    527\u001b[0m }\n\u001b[0;32m    528\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 529\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py:645\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    642\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    644\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 645\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    647\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    648\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py:440\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 440\u001b[0m         resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m            \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m            \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    455\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(conn, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproxy_pool\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    444\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:1377\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1377\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1378\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1379\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:320\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    322\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:281\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 281\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import date, timedelta\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# from get_stats import get_team_stats_dict\n",
    "# from get_matches import get_match_results\n",
    "#from standardization import z_score, stat_std, stat_mean\n",
    "#from available_stats import available_stats\n",
    "\n",
    "\n",
    "# [{'Sacramento Kings': 'Boston Celtics', 'Charlotte Hornets': 'Philadelphia 76ers'}, ['W', 'L']]\n",
    "# team stats is a dataframe\n",
    "def to_dataframe(daily_games, start_date, end_date, season): #, mean_dict, std_dict):\n",
    "    full_dataframe = []\n",
    "    game_number = 0  # counter to match with the correct game\n",
    "    daily_results = daily_games[1]  # win or loss for each game\n",
    "    score = daily_games[2]\n",
    "    game_id = daily_games[3]\n",
    "\n",
    "    for home_team, away_team in daily_games[0].items():  # loops through matchups\n",
    "        home_team_stats = get_team_stats_dict(home_team, start_date, end_date, season)\n",
    "        away_team_stats = get_team_stats_dict(away_team, start_date, end_date, season)\n",
    "\n",
    "        current_game = [home_team, away_team]\n",
    "        \n",
    "        current_game.append(game_id[game_number])\n",
    "\n",
    "        current_game.append(score.pop(0))\n",
    "\n",
    "        for stat, stat_type in available_stats.items():\n",
    "            current_game.append(home_team_stats[stat])\n",
    "        \n",
    "        current_game.append(score.pop(0))\n",
    "\n",
    "        for stat, stat_type in available_stats.items():\n",
    "            current_game.append(away_team_stats[stat])\n",
    "\n",
    "\n",
    "        #for stat, stat_type in available_stats.items():\n",
    "        #    z_score_diff = z_score_difference(home_team_stats[stat], away_team_stats[stat], mean_dict[stat], std_dict[stat])\n",
    "\n",
    "         #   current_game.append(z_score_diff)\n",
    "\n",
    "        if daily_results[game_number] == 'W':\n",
    "            result = 1\n",
    "        else:\n",
    "            result = 0\n",
    "\n",
    "        current_game.append(result)\n",
    "        game_number += 1\n",
    "\n",
    "        print(current_game)\n",
    "\n",
    "        full_dataframe.append(current_game)\n",
    "\n",
    "    return full_dataframe\n",
    "\n",
    "\n",
    "\n",
    "def date_range(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "        \n",
    "\n",
    "def training_set(start_year, start_month, start_day, end_year, end_month, end_day, season, season_start):\n",
    "    start_date = date(start_year, start_month, start_day)\n",
    "    end_date = date(end_year, end_month, end_day)\n",
    "\n",
    "    total_games = []\n",
    "\n",
    "    for single_date in date_range(start_date, end_date):\n",
    "        current_date = single_date.strftime('%m/%d/%Y')\n",
    "        print(current_date)\n",
    "\n",
    "        previous_day = single_date - timedelta(days=1)\n",
    "        previous_day_formatted = previous_day.strftime('%m/%d/%Y')\n",
    "\n",
    "        #mean_std_dictionary = mean_std_dict(season_start, previous_day_formatted, season)\n",
    "        #mean_dict = mean_std_dictionary[0]\n",
    "        #std_dict = mean_std_dictionary[1]\n",
    "\n",
    "        current_day_games = get_match_results(current_date, season)\n",
    "        current_day_games_with_stats = to_dataframe(current_day_games, season_start, previous_day_formatted, season)\n",
    "\n",
    "        for game in current_day_games_with_stats:\n",
    "            game.append(current_date)\n",
    "            total_games.append(game)\n",
    "\n",
    "    print(total_games)\n",
    "    return total_games\n",
    "\n",
    "\n",
    "def make_dataframe(game_list):\n",
    "    games = pd.DataFrame(game_list,\n",
    "                         columns=['Home', 'Away', 'Game_ID', 'H_Score', 'H_W_PCT', 'H_FG_PCT', 'H_FG3_PCT', 'H_FT_PCT',\n",
    "                                  'H_REB', 'H_AST', 'H_TOV', 'H_STL',\n",
    "                                  'H_BLK', 'H_PLUS_MINUS', 'H_OFF_RATING', 'H_DEF_RATING', 'H_TS_PCT', 'A_Score',\n",
    "                                  'A_W_PCT', 'A_FG_PCT', 'A_FG3_PCT',\n",
    "                                  'A_FT_PCT', 'A_REB', 'A_AST', 'A_TOV', 'A_STL',\n",
    "                                  'A_BLK', 'A_PLUS_MINUS', 'A_OFF_RATING', 'A_DEF_RATING', 'A_TS_PCT', 'Result',\n",
    "                                  'Date'])\n",
    "\n",
    "    print(games)\n",
    "    return games\n",
    "\n",
    "\n",
    "def main():\n",
    "    attempts = 10\n",
    "\n",
    "    for i in range(attempts):\n",
    "        try:\n",
    "            all_games = training_set(start_year=2021, start_month=11, start_day=22, end_year=2022, end_month=3, end_day=16,\n",
    "                             season='2021-22', season_start='10/19/2021')\n",
    "            df = make_dataframe(all_games)\n",
    "\n",
    "            print(df)\n",
    "            df.to_csv(r'C:\\Users\\alvaro\\OneDrive\\Documents\\School\\Flatiron\\Projects\\NBA_Prediction_Model\\data\\nba_game_2022_v2.csv', index=False)\n",
    "        except requests.exceptions.ReadTimeout:\n",
    "            if i < attempts - 1:\n",
    "                continue\n",
    "            else:\n",
    "                raise\n",
    "        break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2018-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_2018 = pd.read_csv('./nba_data/nba_df_2018.csv')\n",
    "df_2018['Date'] = pd.to_datetime(df_2018['Date'])\n",
    "df_2018['Season'] = '2018-19'\n",
    "len(df_2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2019 - 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2019 = pd.read_csv('./nba_data/nba_df_2019.csv')\n",
    "df_2019['Date'] = pd.to_datetime(df_2019['Date'])\n",
    "df_2019['Season'] = '2019-20'\n",
    "\n",
    "df_2019_2 = pd.read_csv('./nba_data/nba_df_2019_2.csv')\n",
    "df_2019_2['Date'] = pd.to_datetime(df_2019_2['Date'])\n",
    "df_2019_2['Season'] = '2019-20'\n",
    "\n",
    "print(len(df_2019), len(df_2019_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_2019, df_2019_2]\n",
    "df_2019_final = pd.concat(frames)\n",
    "len(df_2019_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2020 - 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020 = pd.read_csv('./nba_data/nba_df_2020.csv')\n",
    "df_2020['Date'] = pd.to_datetime(df_2020['Date'])\n",
    "df_2020['Season'] = '2020-21'\n",
    "len(df_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_2018, df_2019_final, df_2020]\n",
    "df = pd.concat(frames)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2021 - 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_2022 = pd.read_csv('./nba_data/nba_game_2022_v0.csv')\n",
    "\n",
    "df_2022['Date'] = pd.to_datetime(df_2022['Date'])\n",
    "df_2022['Season'] = '2021-22'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022_1 = pd.read_csv('./nba_data/nba_game_2022_v1.csv')\n",
    "\n",
    "df_2022_1['Date'] = pd.to_datetime(df_2022_1['Date'])\n",
    "df_2022_1['Season'] = '2021-22'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022_2 = pd.read_csv('./nba_data/nba_game_2022_v2.csv')\n",
    "\n",
    "df_2022_2['Date'] = pd.to_datetime(df_2022_2['Date'])\n",
    "df_2022_2['Season'] = '2021-22'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2022_3 = pd.read_csv('./nba_data/nba_game_2022_v3.csv')\n",
    "\n",
    "df_2022_3['Date'] = pd.to_datetime(df_2022_3['Date'])\n",
    "df_2022_3['Season'] = '2021-22'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"length of 2022 data: {len(df_2022) + len(df_2022_1) + len(df_2022_2) + len(df_2022_3)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_2022, df_2022_1, df_2022_2, df_2022_3]\n",
    "df_2022_final = pd.concat(frames)\n",
    "print(f\"Length of 2022 data: {len(df_2022_final)}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frames = [df_2018, df_2019_final, df_2020, df_2022_final]\n",
    "df = pd.concat(frames)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last N Games Win %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "prev_game_df = df[df['Date'] < '12/12/2020'][(df['Home'] == \"Milwaukee Bucks\") | (df['Away'] == 'Milwaukee Bucks')].sort_values(by = 'Date').tail(10)\n",
    "prev_game_df\n",
    "h_df = prev_game_df.iloc[:, range(0,32,31)]\n",
    "\n",
    "h_df = h_df.loc[h_df['Home'] == 'Milwaukee Bucks'] \n",
    "print(h_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_win_pct_last_n_games(team, game_date, df, n):\n",
    "    prev_game_df = df[df['Date'] < game_date][(df['Home'] == team) | (df['Away'] == team)].sort_values(by = 'Date').tail(n)\n",
    "    \n",
    "    wins = 0 \n",
    "    \n",
    "    result_df = prev_game_df.iloc[:, range(0,32,31)]\n",
    "    h_df = result_df.loc[result_df['Home'] == team] \n",
    "    \n",
    "    h_wins = h_df.loc[h_df['Result'] == 1]\n",
    "    \n",
    "    wins += len(h_wins)\n",
    "      \n",
    "    a_df = result_df.loc[result_df['Home'] != team]\n",
    "    a_wins = a_df.loc[a_df['Result'] == 0]\n",
    "    \n",
    "    wins += len(a_wins)\n",
    "\n",
    "    return wins/n\n",
    "get_avg_win_pct_last_n_games('Milwaukee Bucks', '12/12/2020', df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for season in df['Season'].unique() :\n",
    "    \n",
    "    season_stats = df[df['Season'] == season].sort_values(by='Date').reset_index(drop=True)\n",
    "    \n",
    "    for index, row in df.iterrows() : \n",
    "        game_id = row['Game_ID']\n",
    "        game_date = row['Date']\n",
    "        h_team = row['Home']\n",
    "        a_team = row['Away']\n",
    "        \n",
    "        df.loc[index,'Home_W_Pct_10'] = get_avg_win_pct_last_n_games(h_team, game_date, df, 10)\n",
    "        \n",
    "        df.loc[index,'Away_W_Pct_10'] = get_avg_win_pct_last_n_games(a_team, game_date, df, 10)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['Season'] == '2020-21'].tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELO Rating\n",
    "- every team starts with a 1500\n",
    "$$R_{i+1} = k * (S_{team} - E_{team} + R_{i})$$\n",
    "- S team is 1 if the team wins and 0 if they lose\n",
    "- E team is the expected win probability of the team \n",
    "$$E_{team} = \\frac{1}{1+10^{\\frac{opp\\_elo - team\\_elo}{400}}}$$\n",
    "- k is a moving constant that depends on margin of victory and difference in Elo ratings\n",
    "$$k = 20\\frac{(MOV_{winner} + 3)^{0.8}}{7.5 + 0.006(elo\\_difference_{winner})} $$\n",
    "- team year by year carryover \n",
    "$$(R * 0.75) + (0.25 * 1505)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Home and road team win probabilities implied by Elo ratings and home court adjustment \n",
    "import math\n",
    "import time\n",
    "def win_probs(home_elo, away_elo, home_court_advantage) :\n",
    "    h = math.pow(10, home_elo/400)\n",
    "    r = math.pow(10, away_elo/400)\n",
    "    a = math.pow(10, home_court_advantage/400) \n",
    "\n",
    "    denom = r + a*h\n",
    "    home_prob = a*h / denom\n",
    "    away_prob = r / denom \n",
    "  \n",
    "    return home_prob, away_prob\n",
    "\n",
    "  #odds the home team will win based on elo ratings and home court advantage\n",
    "\n",
    "def home_odds_on(home_elo, away_elo, home_court_advantage) :\n",
    "    h = math.pow(10, home_elo/400)\n",
    "    r = math.pow(10, away_elo/400)\n",
    "    a = math.pow(10, home_court_advantage/400)\n",
    "    return a*h/r\n",
    "\n",
    "#this function determines the constant used in the elo rating, based on margin of victory and difference in elo ratings\n",
    "def elo_k(MOV, elo_diff):\n",
    "    k = 20 # Optimal K is 20 https://fivethirtyeight.com/features/how-we-calculate-nba-elo-ratings/\n",
    "    if MOV>0:\n",
    "        multiplier=(MOV+3)**(0.8)/(7.5+0.006*(elo_diff))\n",
    "    else:\n",
    "        multiplier=(-MOV+3)**(0.8)/(7.5+0.006*(-elo_diff))\n",
    "    return k*multiplier\n",
    "\n",
    "\n",
    "# Updates the home and away teams elo ratings after a game \n",
    "\n",
    "def update_elo(home_score, away_score, home_elo, away_elo, home_court_advantage) :\n",
    "    home_prob, away_prob = win_probs(home_elo, away_elo, home_court_advantage) \n",
    "\n",
    "    if (home_score - away_score > 0) :\n",
    "        home_win = 1 \n",
    "        away_win = 0 \n",
    "    else :\n",
    "        home_win = 0 \n",
    "        away_win = 1 \n",
    "  \n",
    "    k = elo_k(home_score - away_score, home_elo - away_elo)\n",
    "\n",
    "    updated_home_elo = home_elo + k * (home_win - home_prob) \n",
    "    updated_away_elo = away_elo + k * (away_win - away_prob)\n",
    "    \n",
    "    return updated_home_elo, updated_away_elo\n",
    "\n",
    "\n",
    "# Takes into account prev season elo\n",
    "# The reason we revert to a mean of 1505 rather than 1500 is that \n",
    "# there are liable to be a couple of relatively recent expansion teams in the league at any given time\n",
    "def get_prev_elo(team, date, season, team_stats, elo_df) :\n",
    "    prev_game = team_stats[team_stats['Date'] < game_date][(team_stats['Home'] == team) | (team_stats['Away'] == team)].sort_values(by = 'Date').tail(1).iloc[0] \n",
    "\n",
    "    if team == prev_game['Home'] :\n",
    "        elo_rating = elo_df[elo_df['Game_ID'] == prev_game['Game_ID']]['H_Team_Elo_After'].values[0]\n",
    "    else :\n",
    "        elo_rating = elo_df[elo_df['Game_ID'] == prev_game['Game_ID']]['A_Team_Elo_After'].values[0]\n",
    "  \n",
    "    if prev_game['Season'] != season :\n",
    "        return (0.75 * elo_rating) + (0.25 * 1505) # Year-to-Year Carry-Over\n",
    "    else :\n",
    "        return elo_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by = 'Date', inplace = True)\n",
    "df.reset_index(inplace=True, drop = True)\n",
    "elo_df = pd.DataFrame(columns=['Game_ID', 'H_Team', 'A_Team', 'H_Team_Elo_Before', 'A_Team_Elo_Before', 'H_Team_Elo_After', 'A_Team_Elo_After'])\n",
    "teams_elo_df = pd.DataFrame(columns=['Game_ID','Team', 'Elo', 'Date', 'Where_Played', 'Season']) \n",
    "\n",
    "for index, row in df.iterrows(): \n",
    "    game_id = row['Game_ID']\n",
    "    game_date = row['Date']\n",
    "    season = row['Season']\n",
    "    h_team, a_team = row['Home'], row['Away']\n",
    "    h_score, a_score = row['H_Score'], row['A_Score'] \n",
    "\n",
    "    if (h_team not in elo_df['H_Team'].values and h_team not in elo_df['A_Team'].values) :\n",
    "        h_team_elo_before = 1500\n",
    "    else :\n",
    "        h_team_elo_before = get_prev_elo(h_team, game_date, season, df, elo_df)\n",
    "\n",
    "    if (a_team not in elo_df['H_Team'].values and a_team not in elo_df['A_Team'].values) :\n",
    "        a_team_elo_before = 1500\n",
    "    else :\n",
    "        a_team_elo_before = get_prev_elo(a_team, game_date, season, df, elo_df)\n",
    "\n",
    "    h_team_elo_after, a_team_elo_after = update_elo(h_score, a_score, h_team_elo_before, a_team_elo_before, 69)\n",
    "\n",
    "    new_row = {'Game_ID': game_id, 'H_Team': h_team, 'A_Team': a_team, 'H_Team_Elo_Before': h_team_elo_before, 'A_Team_Elo_Before': a_team_elo_before, \\\n",
    "                                                                        'H_Team_Elo_After' : h_team_elo_after, 'A_Team_Elo_After': a_team_elo_after}\n",
    "    teams_row_one = {'Game_ID': game_id,'Team': h_team, 'Elo': h_team_elo_before, 'Date': game_date, 'Where_Played': 'Home', 'Season': season}\n",
    "    teams_row_two = {'Game_ID': game_id,'Team': a_team, 'Elo': a_team_elo_before, 'Date': game_date, 'Where_Played': 'Away', 'Season': season}\n",
    "  \n",
    "    elo_df = elo_df.append(new_row, ignore_index = True)\n",
    "    teams_elo_df = teams_elo_df.append(teams_row_one, ignore_index=True)\n",
    "    teams_elo_df = teams_elo_df.append(teams_row_two, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#teams_elo_df.set_index([\"Team\"], append=True)\n",
    "#dataset = teams_elo_df.pivot(index=\"Team\",values=\"Elo\", columns=\"Date\")\n",
    "dates = list(set([d.strftime(\"%m-%d-%Y\") for d in teams_elo_df[\"Date\"]]))\n",
    "dates = sorted(dates, key=lambda x: time.strptime(x, '%m-%d-%Y'))\n",
    "teams = df[\"Away\"]\n",
    "dataset = pd.DataFrame(columns=dates)\n",
    "dataset[\"Team\"] = teams.drop_duplicates()\n",
    "dataset = dataset.set_index(\"Team\")\n",
    "\n",
    "for index, row in teams_elo_df.iterrows():\n",
    "    date = row[\"Date\"].strftime(\"%m-%d-%Y\")\n",
    "    team = row[\"Team\"]\n",
    "    elo = row[\"Elo\"]\n",
    "    dataset[date][team] = elo\n",
    "\n",
    "teams_elo_df['Elo'] = teams_elo_df['Elo'].astype(float)\n",
    "\n",
    "elo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(elo_df.drop(columns=['H_Team', 'A_Team']), on ='Game_ID')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization and Z Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Different Models - No Z Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'C:\\Users\\alvaro\\OneDrive\\Documents\\School\\Flatiron\\Projects\\NBA_Model\\nba_data\\nba_df_final_test.csv', index = False)\n",
    "print(f'The final dataset consists of three seasons and {len(df)} games.')\n",
    "df = df.reset_index(drop=True)\n",
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df = df.drop(labels=['H_Team_Elo_After', 'A_Team_Elo_After'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"H_Team_Elo_Before\"] = df.H_Team_Elo_Before.astype(float)\n",
    "df[\"A_Team_Elo_Before\"] = df.A_Team_Elo_Before.astype(float)\n",
    "final_df = df.drop(['Home', 'Away', 'Game_ID', 'H_Score', 'A_Score', 'Date', 'Season'], axis=1)\n",
    "#final_df.head()\n",
    "#final_df.columns\n",
    "#final_df.info()\n",
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(r'C:\\Users\\alvaro\\OneDrive\\Documents\\School\\Flatiron\\Projects\\NBA_Model\\nba_data\\nbas.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "final_df = pd.read_csv('./nba_data/nbas.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(44, 34))\n",
    "\n",
    "sns.heatmap(final_df.corr(), annot=True);\n",
    "\n",
    "# correlation = final_df[['H_W_PCT', 'H_REB', 'H_AST',\n",
    "#        'H_TOV', 'H_STL', 'H_BLK', 'H_PLUS_MINUS', 'H_OFF_RATING',\n",
    "#        'H_DEF_RATING', 'H_TS_PCT', 'H_Team_Elo_Before', 'Home_W_Pct_10', 'Result']].corr()\n",
    "# sns.heatmap(final_df['Result'].corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.corr()['Result'].abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    fig, ax = plt.subplots(figsize=(44, 34))\n",
    "    \n",
    "    mask = np.triu(np.ones_like(final_df.corr(), dtype=bool))\n",
    "    ax = sns.heatmap(abs(final_df.corr()),mask=mask,annot=True)\n",
    "    fig.savefig('images/Corelation_Heatmap');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#import final dataset\n",
    "#final_df = pd.read_csv('')\n",
    "\n",
    "#drop non numeric columns\n",
    "#df.drop(columns = ['Home', 'Away', 'Game_ID', 'Date', 'Season'], axis = 1, inplace = True )\n",
    "\n",
    "X = final_df.drop(columns = 'Result')\n",
    "\n",
    "y = final_df['Result']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print(f'X train shape: {X_train.shape}')\n",
    "print(f'X test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"Raw Counts \n",
    "{final_df[\"Result\"].value_counts()}\\n\n",
    "Percentages \n",
    "{final_df[\"Result\"].value_counts(normalize=True)}\n",
    "\n",
    "\n",
    "We would get an accuracy score of 0.566312 with a baseline model, i.e. about 56.6% accuracy\n",
    "\n",
    "This is because about 56.6% of the results are wins\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"Train percent wins\n",
    "{y_train.value_counts(normalize=True)}\\n\"\"\")\n",
    "\n",
    "print(f\"\"\"Test percent wins: \n",
    "{y_test.value_counts(normalize=True)}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "class ModelWithCV():\n",
    "    '''Structure to save the model and more easily see its crossvalidation'''\n",
    "    \n",
    "    def __init__(self, model, model_name, X, y, cv_now=True):\n",
    "        self.model = model\n",
    "        self.name = model_name\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        # For CV results\n",
    "        self.cv_results = None\n",
    "        self.cv_mean = None\n",
    "        self.cv_median = None\n",
    "        self.cv_std = None\n",
    "        #\n",
    "        if cv_now:\n",
    "            self.cross_validate()\n",
    "        \n",
    "    def cross_validate(self, X=None, y=None, kfolds=10):\n",
    "        '''\n",
    "        Perform cross-validation and return results.\n",
    "        \n",
    "        Args: \n",
    "          X:\n",
    "            Optional; Training data to perform CV on. Otherwise use X from object\n",
    "          y:\n",
    "            Optional; Training data to perform CV on. Otherwise use y from object\n",
    "          kfolds:\n",
    "            Optional; Number of folds for CV (default is 10)  \n",
    "        '''\n",
    "        \n",
    "        cv_X = X if X else self.X\n",
    "        cv_y = y if y else self.y\n",
    "\n",
    "        self.cv_results = cross_val_score(self.model, cv_X, cv_y, cv=kfolds)\n",
    "        self.cv_mean = np.mean(self.cv_results)\n",
    "        self.cv_median = np.median(self.cv_results)\n",
    "        self.cv_std = np.std(self.cv_results)\n",
    "\n",
    "        \n",
    "    def print_cv_summary(self):\n",
    "        cv_summary = (\n",
    "        f'''CV Results for `{self.name}` model:\n",
    "            {self.cv_mean:.5f}  {self.cv_std:.5f} accuracy\n",
    "        ''')\n",
    "        print(cv_summary)\n",
    "\n",
    "        \n",
    "    def plot_cv(self, ax):\n",
    "        '''\n",
    "        Plot the cross-validation values using the array of results and given \n",
    "        Axis for plotting.\n",
    "        '''\n",
    "        ax.set_title(f'CV Results for `{self.name}` Model')\n",
    "        # Thinner violinplot with higher bw\n",
    "        sns.violinplot(y=self.cv_results, ax=ax, bw=.4)\n",
    "        sns.swarmplot(\n",
    "                y=self.cv_results,\n",
    "                color='orange',\n",
    "                size=10,\n",
    "                alpha= 0.8,\n",
    "                ax=ax\n",
    "        )\n",
    "\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dummy/Baseliner\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "estimator = DummyRegressor(strategy='mean')\n",
    "\n",
    "# Create Dummy/Baseliner\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('estimator', DummyRegressor(strategy='mean'))\n",
    "])\n",
    "\n",
    "cv = ModelWithCV(pipe, 'estimator', X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "\n",
    "cv.plot_cv(ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.print_cv_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_pipe.print_cv_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant class and function\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    #('scaler', StandardScaler()),\n",
    "    #('pca', PCA(n_components=4)),\n",
    "    ('estimator', LogisticRegression(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {}\n",
    "param_grid['estimator__C'] = np.logspace(-4, 4, 50)\n",
    "param_grid['estimator__solver'] = ['liblinear']\n",
    "param_grid['estimator__penalty'] = ['l1', 'l2']\n",
    "param_grid['estimator__class_weight'] = ['balanced', None]\n",
    "#params['logreg__n_jobs'] = [1]\n",
    "param_grid['estimator__max_iter'] = [1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=10, return_train_score=True, scoring='accuracy', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest - Random Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('estimator', RandomForestClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {}\n",
    "param_grid['estimator__n_estimators'] = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "param_grid['estimator__max_features'] = ['auto', 'sqrt']\n",
    "param_grid['estimator__max_depth'] = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "param_grid['estimator__min_samples_split'] = [2, 5, 10]\n",
    "param_grid['estimator__min_samples_leaf'] = [1, 2, 4]\n",
    "param_grid['estimator__bootstrap'] = [True, False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "grid_search = RandomizedSearchCV(estimator=pipe, \n",
    "                                 param_distributions=param_grid, \n",
    "                                 cv=10, return_train_score=True, \n",
    "                                 scoring='accuracy', \n",
    "                                 n_iter=100, \n",
    "                                 random_state=42, \n",
    "                                 n_jobs = -1, \n",
    "                                 verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest - Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('estimator', RandomForestClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {}\n",
    "param_grid['estimator__n_estimators'] = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
    "param_grid['estimator__max_features'] = ['auto', 'sqrt']\n",
    "param_grid['estimator__max_depth'] = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "param_grid['estimator__min_samples_split'] = [2, 5, 10]\n",
    "param_grid['estimator__min_samples_leaf'] = [1, 2, 4]\n",
    "param_grid['estimator__bootstrap'] = [True, False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipe, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=10, \n",
    "                           return_train_score=True, \n",
    "                           scoring='accuracy',\n",
    "                           n_jobs = -1,\n",
    "                           verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('estimator', GaussianNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {}\n",
    "param_grid['estimator__var_smoothing'] = np.logspace(0,-9, num=100)\n",
    "#param_grid['estimator__var_smoothing'] = [1e-11, 1e-10, 1e-9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "grid_search = RandomizedSearchCV(estimator=pipe, \n",
    "                                 param_distributions=param_grid, \n",
    "                                 cv=10, \n",
    "                                 return_train_score=True, \n",
    "                                 scoring='accuracy', \n",
    "                                 random_state=42,\n",
    "                                 n_jobs = -1,\n",
    "                                 verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('estimator', xgboost.XGBRegressor(random_state=42, objective='reg:squarederror'))\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {}\n",
    "param_grid['estimator__min_child_weight'] = [1, 5, 10],\n",
    "param_grid['estimator__gamma'] = [0.5, 1, 1.5, 2, 5],\n",
    "param_grid['estimator__subsample'] = [0.6, 0.8, 1.0],\n",
    "param_grid['estimator__colsample_bytree'] = [0.6, 0.8, 1.0],\n",
    "param_grid['estimator__max_depth'] = [3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipe, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=10, \n",
    "                           return_train_score=True, \n",
    "                           scoring='accuracy',\n",
    "                           n_jobs = -1,\n",
    "                           verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#script to test the effectivenes of each model, uses default parameters\n",
    "#test six different classification models \n",
    "def run_exps(X_train, y_train, X_test, y_test) :\n",
    "    '''\n",
    "    Lightweight script to test many models and find winners\n",
    "    :param X_train: training split\n",
    "    :param y_train: training target vector\n",
    "    :param X_test: test split\n",
    "    :param y_test: test target vector\n",
    "    :return: DataFrame of predictions\n",
    "    '''\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    models = [\n",
    "          ('LogReg', LogisticRegression()), \n",
    "          ('RF', RandomForestClassifier()),\n",
    "          ('KNN', KNeighborsClassifier()),\n",
    "          ('SVM', SVC()), \n",
    "          ('GNB', GaussianNB()),\n",
    "          ('XGB', XGBClassifier())\n",
    "        ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    names = []\n",
    "    \n",
    "    scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted', 'roc_auc']\n",
    "    \n",
    "    target_names = ['win', 'loss']\n",
    "    \n",
    "    for name, model in models:\n",
    "        \n",
    "        kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=90210)\n",
    "        cv_results = model_selection.cross_validate(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "        clf = model.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        print(name)\n",
    "        print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "        \n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        \n",
    "        this_df = pd.DataFrame(cv_results)\n",
    "        this_df['model'] = name\n",
    "        dfs.append(this_df)\n",
    "        \n",
    "    final = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    return final\n",
    "final = run_exps(X_train, y_train, X_test, y_test)\n",
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstraps = []\n",
    "for model in list(set(final.model.values)):\n",
    "    model_df = final.loc[final.model == model]\n",
    "    bootstrap = model_df.sample(n=30, replace=True)\n",
    "    bootstraps.append(bootstrap)\n",
    "        \n",
    "bootstrap_df = pd.concat(bootstraps, ignore_index=True)\n",
    "results_long = pd.melt(bootstrap_df,id_vars=['model'],var_name='metrics', value_name='values')\n",
    "time_metrics = ['fit_time','score_time'] # fit time metrics\n",
    "## PERFORMANCE METRICS\n",
    "results_long_nofit = results_long.loc[~results_long['metrics'].isin(time_metrics)] # get df without fit data\n",
    "results_long_nofit = results_long_nofit.sort_values(by='values')\n",
    "## TIME METRICS\n",
    "results_long_fit = results_long.loc[results_long['metrics'].isin(time_metrics)] # df with fit data\n",
    "results_long_fit = results_long_fit.sort_values(by='values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(20, 12))\n",
    "sns.set(font_scale=2.5)\n",
    "g = sns.boxplot(x=\"model\", y=\"values\", hue=\"metrics\", data=results_long_nofit, palette=\"Set3\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('Comparison of Model by Classification Metric')\n",
    "plt.savefig('./benchmark_models_performance.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "sns.set(font_scale=2.5)\n",
    "g = sns.boxplot(x=\"model\", y=\"values\", hue=\"metrics\", data=results_long_fit, palette=\"Set3\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('Comparison of Model by Fit and Score Time')\n",
    "plt.savefig('./benchmark_models_time.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = list(set(results_long_nofit.metrics.values))\n",
    "bootstrap_df.groupby(['model'])[metrics].agg([np.std, np.mean])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Selected Model\n",
    "- grid search for parameters \n",
    "- Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian NB only has one parameter 'var_smoothing'\n",
    "# Portion of the largest variance of all features that is added to variances for calculation stability.\n",
    "# Number of different combinations of parameters \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "nb_classifier = GaussianNB()\n",
    "\n",
    "target_names = ['Win', 'Loss']\n",
    "\n",
    "params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=90210)\n",
    "\n",
    "gs_NB = GridSearchCV(estimator=nb_classifier, \n",
    "                 param_grid=params_NB, \n",
    "                 cv=kfold,   \n",
    "                 verbose=1, \n",
    "                 scoring='accuracy', n_jobs=-1) \n",
    "\n",
    "gs_NB.fit(X_train, y_train)\n",
    "\n",
    "best_gs_grid = gs_NB.best_estimator_\n",
    "best_gs_grid.fit(X_train, y_train)\n",
    "y_pred_best_gs = best_gs_grid.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_best_gs, target_names=target_names))\n",
    "gs_NB.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusionMatrix = confusion_matrix(y_test, y_pred_best_gs)\n",
    "print(confusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionMatrix = confusion_matrix(y_test, y_pred_best_gs)  \n",
    "\n",
    "    # Code below prints model accuracy information\n",
    "print('Coefficient Information:')\n",
    "\n",
    "for i in range(len(featureColumns)):  \n",
    "\n",
    "    logregCoefficients = logreg.coef_\n",
    "\n",
    "    currentFeature = featureColumns[i]\n",
    "    currentCoefficient = logregCoefficients[0][i]\n",
    "\n",
    "    print(currentFeature + ': ' + str(currentCoefficient))\n",
    "\n",
    "print('----------------------------------')\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(Y_test, Y_pred))\n",
    "print(\"Precision:\", metrics.precision_score(Y_test, Y_pred))\n",
    "print(\"Recall:\", metrics.recall_score(Y_test, Y_pred))\n",
    "\n",
    "print('----------------------------------')\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(confusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Saves the model in folder to be used in future\n",
    "# filename should be end in '.pkl'\n",
    "def save_model(model, filename):\n",
    "\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
