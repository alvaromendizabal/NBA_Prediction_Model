{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = {\n",
    "        \"Atlanta Hawks\": 1610612737,\n",
    "        \"Boston Celtics\": 1610612738,\n",
    "        \"Brooklyn Nets\": 1610612751,\n",
    "        \"Charlotte Bobcats\": 1610612766,\n",
    "        \"Charlotte Hornets\": 1610612766,\n",
    "        \"Chicago Bulls\": 1610612741,\n",
    "        \"Cleveland Cavaliers\": 1610612739,\n",
    "        \"Dallas Mavericks\": 1610612742,\n",
    "        \"Denver Nuggets\": 1610612743,\n",
    "        \"Detroit Pistons\": 1610612765,\n",
    "        \"Golden State Warriors\": 1610612744,\n",
    "        \"Houston Rockets\": 1610612745,\n",
    "        \"Indiana Pacers\": 1610612754,\n",
    "        \"LA Clippers\": 1610612746,\n",
    "        \"Los Angeles Clippers\": 1610612746,\n",
    "        \"Los Angeles Lakers\": 1610612747,\n",
    "        \"Memphis Grizzlies\": 1610612763,\n",
    "        \"Miami Heat\": 1610612748,\n",
    "        \"Milwaukee Bucks\": 1610612749,\n",
    "        \"Minnesota Timberwolves\": 1610612750,\n",
    "        \"New Jersey Nets\": 1610612751,\n",
    "        \"New Orleans Hornets\": 1610612740,\n",
    "        \"New Orleans Pelicans\": 1610612740,\n",
    "        \"New York Knicks\": 1610612752,\n",
    "        \"Oklahoma City Thunder\": 1610612760,\n",
    "        \"Orlando Magic\": 1610612753,\n",
    "        \"Philadelphia 76ers\": 1610612755,\n",
    "        \"Phoenix Suns\": 1610612756,\n",
    "        \"Portland Trail Blazers\": 1610612757,\n",
    "        \"Sacramento Kings\": 1610612758,\n",
    "        \"San Antonio Spurs\": 1610612759,\n",
    "        \"Toronto Raptors\": 1610612761,\n",
    "        \"Utah Jazz\": 1610612762,\n",
    "        \"Washington Wizards\": 1610612764,\n",
    "    }\n",
    "\n",
    "available_stats = {'W_PCT': 'Base',\n",
    "                   'FG_PCT': 'Base',\n",
    "                   'FG3_PCT': 'Base',\n",
    "                   'FT_PCT': 'Base',\n",
    "                   'REB': 'Base',\n",
    "                   'AST': 'Base',\n",
    "                   'TOV': 'Base',\n",
    "                   'STL': 'Base',\n",
    "                   'BLK': 'Base',\n",
    "                   'PLUS_MINUS': 'Base',\n",
    "                   'OFF_RATING': 'Advanced',\n",
    "                   'DEF_RATING': 'Advanced',\n",
    "                   'TS_PCT': 'Advanced'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_stats.py gets the team data for the model\n",
    "\n",
    "# from team_names import teams\n",
    "import nba_api\n",
    "from nba_api.stats.endpoints import teamdashboardbygeneralsplits, leaguedashteamstats\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def get_team_stats_dict(team, start_date, end_date, season='2021-22'):\n",
    "    \"\"\"\n",
    "    Returns the stats for the specified team in a dataframe, default year is 2021-22\n",
    "    :param team: Day of games scheduled in form 'mm/dd/yyyy'\n",
    "    :param season: Day of games scheduled in form 'mm/dd/yyyy'\n",
    "    :return: A dictionary of game matchups {home_team:[away_team]}\n",
    "    \"\"\"\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    \n",
    "    general_team_info = teamdashboardbygeneralsplits.TeamDashboardByGeneralSplits(team_id=teams[team],\n",
    "                                                                                  per_mode_detailed='Per100Possessions',\n",
    "                                                                                  season=season,\n",
    "                                                                                  date_from_nullable=start_date,\n",
    "                                                                                  date_to_nullable=end_date,\n",
    "                                                                                  timeout=120)\n",
    "    general_team_dict = general_team_info.get_normalized_dict()\n",
    "    general_team_dashboard = general_team_dict['OverallTeamDashboard'][0]\n",
    "\n",
    "\n",
    "    win_percentage = general_team_dashboard['W_PCT']\n",
    "    fg_percentage = general_team_dashboard['FG_PCT']\n",
    "    fg3_percentage = general_team_dashboard['FG3_PCT']\n",
    "    ft_percentage = general_team_dashboard['FT_PCT']\n",
    "    rebounds = general_team_dashboard['REB']\n",
    "    assists = general_team_dashboard['AST']\n",
    "    turnovers = general_team_dashboard['TOV']\n",
    "    steals = general_team_dashboard['STL']\n",
    "    blocks = general_team_dashboard['BLK']\n",
    "    plus_minus = general_team_dashboard['PLUS_MINUS']\n",
    "\n",
    "    advanced_team_info = teamdashboardbygeneralsplits.TeamDashboardByGeneralSplits(team_id=teams[team],\n",
    "                                                                                   measure_type_detailed_defense='Advanced',\n",
    "                                                                                   season=season,\n",
    "                                                                                   date_from_nullable=start_date,\n",
    "                                                                                   date_to_nullable=end_date,\n",
    "                                                                                   timeout=120)\n",
    "    advanced_team_dict = advanced_team_info.get_normalized_dict()\n",
    "    advanced_team_dashboard = advanced_team_dict['OverallTeamDashboard'][0]\n",
    "\n",
    "    offensive_rating = advanced_team_dashboard['OFF_RATING']\n",
    "    defensive_rating = advanced_team_dashboard['DEF_RATING']\n",
    "    true_shooting_percentage = advanced_team_dashboard['TS_PCT']\n",
    "\n",
    "    all_stats_dict = {\n",
    "        'W_PCT': win_percentage,\n",
    "        'FG_PCT': fg_percentage,\n",
    "        'FG3_PCT': fg3_percentage,\n",
    "        'FT_PCT': ft_percentage,\n",
    "        'REB': rebounds,\n",
    "        'AST': assists,\n",
    "        'TOV': turnovers,\n",
    "        'STL': steals,\n",
    "        'BLK': blocks,\n",
    "        'PLUS_MINUS': plus_minus,\n",
    "        'OFF_RATING': offensive_rating,\n",
    "        'DEF_RATING': defensive_rating,\n",
    "        'TS_PCT': true_shooting_percentage\n",
    "    }\n",
    "\n",
    "    return all_stats_dict\n",
    "\n",
    "\n",
    "get_team_stats_dict('Golden State Warriors', '10/19/2021', '04/10/2022', '2021-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get_matches.py gets the daily matches for a specific date and the results of the games\n",
    "\n",
    "from nba_api.stats.endpoints import leaguegamelog, scoreboard, leaguestandings\n",
    "#from team_names import teams\n",
    "\n",
    "\n",
    "\n",
    "def get_match_results(date, season):\n",
    "    \"\"\"\n",
    "    Returns the matchup and result of the game\n",
    "\n",
    "    :param date: Day of games scheduled in form 'mm/dd/yyyy'\n",
    "    :param season: Season in form of 'yyyy-yy'\n",
    "    :return: [{Boston Celtics: Los Angeles Lakers}], ['W']\n",
    "    \"\"\"\n",
    "\n",
    "    game_log = leaguegamelog.LeagueGameLog(season=season, league_id='00', date_from_nullable=date,\n",
    "                                           date_to_nullable=date, season_type_all_star='Regular Season', timeout=120)\n",
    "    game_log_dict = game_log.get_normalized_dict()\n",
    "    list_of_teams = game_log_dict['LeagueGameLog']\n",
    "\n",
    "    daily_match = {}\n",
    "    win_loss = []\n",
    "    score = []\n",
    "    game_id = []\n",
    "\n",
    "    for i in range(0, len(list_of_teams), 2):\n",
    "\n",
    "        if '@' in list_of_teams[i]['MATCHUP']:\n",
    "\n",
    "            away_team = list_of_teams[i]['TEAM_NAME']\n",
    "            home_team = list_of_teams[i + 1]['TEAM_NAME']\n",
    "\n",
    "            win_loss.append(list_of_teams[i + 1]['WL'])\n",
    "\n",
    "            game_id.append(list_of_teams[i + 1]['GAME_ID'])\n",
    "\n",
    "            score.append(list_of_teams[i + 1]['PTS'])\n",
    "            score.append(list_of_teams[i]['PTS'])\n",
    "\n",
    "        else:\n",
    "            away_team = list_of_teams[i + 1]['TEAM_NAME']\n",
    "            home_team = list_of_teams[i]['TEAM_NAME']\n",
    "\n",
    "            win_loss.append(list_of_teams[i]['WL'])\n",
    "\n",
    "            game_id.append(list_of_teams[i]['GAME_ID'])\n",
    "\n",
    "            score.append(list_of_teams[i]['PTS'])\n",
    "            score.append(list_of_teams[i + 1]['PTS'])\n",
    "\n",
    "        daily_match.update({home_team: away_team})\n",
    "\n",
    "    match_results = [daily_match, win_loss, score, game_id]\n",
    "\n",
    "    return match_results\n",
    "\n",
    "\n",
    "def get_daily_matches(date):\n",
    "    \"\"\"\n",
    "    This method creates a dictionary of daily game matchups.\n",
    "\n",
    "    :param date: Day of games scheduled in form 'mm/dd/yyyy'\n",
    "    :return: A dictionary of game matchups {home_team:away_team}\n",
    "    \"\"\"\n",
    "\n",
    "    daily_match = scoreboard.Scoreboard(league_id='00', game_date=date, timeout=120)\n",
    "    daily_match_dict = daily_match.get_normalized_dict()\n",
    "    games = daily_match_dict['GameHeader']\n",
    "\n",
    "    match = {}\n",
    "\n",
    "    for game in games:\n",
    "\n",
    "        home_team_id = game['HOME_TEAM_ID']\n",
    "\n",
    "        for team, team_id in teams.items():\n",
    "            if team_id == home_team_id:\n",
    "                home_team = team\n",
    "\n",
    "        away_team_id = game['VISITOR_TEAM_ID']\n",
    "\n",
    "        for team, team_id in teams.items():\n",
    "            if team_id == away_team_id:\n",
    "                away_team = team\n",
    "\n",
    "        match.update({home_team: away_team})\n",
    "\n",
    "    return match\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(f\"\"\"'get_daily_matches' returns a dictionary of the games on a specified date\\n{get_daily_matches('12/25/22')}\\n\"\"\")\n",
    "    print(f\"\"\"'get_match_results' returns the matchup plus the result\\n{get_match_results('10/19/2021', '2021-22')}\"\"\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# from get_stats import get_team_stats_dict\n",
    "# from get_matches import get_match_results\n",
    "#from standardization import z_score, stat_std, stat_mean\n",
    "#from available_stats import available_stats\n",
    "\n",
    "\n",
    "# [{'Sacramento Kings': 'Boston Celtics', 'Charlotte Hornets': 'Philadelphia 76ers'}, ['W', 'L']]\n",
    "# team stats is a dataframe\n",
    "def to_dataframe(daily_games, start_date, end_date, season): #, mean_dict, std_dict):\n",
    "    full_dataframe = []\n",
    "    game_number = 0  # counter to match with the correct game\n",
    "    daily_results = daily_games[1]  # win or loss for each game\n",
    "    score = daily_games[2]\n",
    "    game_id = daily_games[3]\n",
    "\n",
    "    for home_team, away_team in daily_games[0].items():  # loops through matchups\n",
    "        home_team_stats = get_team_stats_dict(home_team, start_date, end_date, season)\n",
    "        away_team_stats = get_team_stats_dict(away_team, start_date, end_date, season)\n",
    "\n",
    "        current_game = [home_team, away_team]\n",
    "        \n",
    "        current_game.append(game_id[game_number])\n",
    "\n",
    "        current_game.append(score.pop(0))\n",
    "\n",
    "        for stat, stat_type in available_stats.items():\n",
    "            current_game.append(home_team_stats[stat])\n",
    "        \n",
    "        current_game.append(score.pop(0))\n",
    "\n",
    "        for stat, stat_type in available_stats.items():\n",
    "            current_game.append(away_team_stats[stat])\n",
    "\n",
    "\n",
    "        #for stat, stat_type in available_stats.items():\n",
    "        #    z_score_diff = z_score_difference(home_team_stats[stat], away_team_stats[stat], mean_dict[stat], std_dict[stat])\n",
    "\n",
    "         #   current_game.append(z_score_diff)\n",
    "\n",
    "        if daily_results[game_number] == 'W':\n",
    "            result = 1\n",
    "        else:\n",
    "            result = 0\n",
    "\n",
    "        current_game.append(result)\n",
    "        game_number += 1\n",
    "\n",
    "        print(current_game)\n",
    "\n",
    "        full_dataframe.append(current_game)\n",
    "\n",
    "    return full_dataframe\n",
    "\n",
    "\n",
    "\n",
    "def date_range(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "        \n",
    "\n",
    "def training_set(start_year, start_month, start_day, end_year, end_month, end_day, season, season_start):\n",
    "    start_date = date(start_year, start_month, start_day)\n",
    "    end_date = date(end_year, end_month, end_day)\n",
    "\n",
    "    total_games = []\n",
    "\n",
    "    for single_date in date_range(start_date, end_date):\n",
    "        current_date = single_date.strftime('%m/%d/%Y')\n",
    "        print(current_date)\n",
    "\n",
    "        previous_day = single_date - timedelta(days=1)\n",
    "        previous_day_formatted = previous_day.strftime('%m/%d/%Y')\n",
    "\n",
    "        #mean_std_dictionary = mean_std_dict(season_start, previous_day_formatted, season)\n",
    "        #mean_dict = mean_std_dictionary[0]\n",
    "        #std_dict = mean_std_dictionary[1]\n",
    "\n",
    "        current_day_games = get_match_results(current_date, season)\n",
    "        current_day_games_with_stats = to_dataframe(current_day_games, season_start, previous_day_formatted, season)\n",
    "\n",
    "        for game in current_day_games_with_stats:\n",
    "            game.append(current_date)\n",
    "            total_games.append(game)\n",
    "\n",
    "    print(total_games)\n",
    "    return total_games\n",
    "\n",
    "\n",
    "def make_dataframe(game_list):\n",
    "    games = pd.DataFrame(game_list,\n",
    "                         columns=['Home', 'Away', 'Game_ID', 'H_Score', 'H_W_PCT', 'H_FG_PCT', 'H_FG3_PCT', 'H_FT_PCT',\n",
    "                                  'H_REB', 'H_AST', 'H_TOV', 'H_STL',\n",
    "                                  'H_BLK', 'H_PLUS_MINUS', 'H_OFF_RATING', 'H_DEF_RATING', 'H_TS_PCT', 'A_Score',\n",
    "                                  'A_W_PCT', 'A_FG_PCT', 'A_FG3_PCT',\n",
    "                                  'A_FT_PCT', 'A_REB', 'A_AST', 'A_TOV', 'A_STL',\n",
    "                                  'A_BLK', 'A_PLUS_MINUS', 'A_OFF_RATING', 'A_DEF_RATING', 'A_TS_PCT', 'Result',\n",
    "                                  'Date'])\n",
    "\n",
    "    print(games)\n",
    "    return games\n",
    "\n",
    "\n",
    "def main():\n",
    "    attempts = 10\n",
    "\n",
    "    for i in range(attempts):\n",
    "        try:\n",
    "            all_games = training_set(start_year=2017, start_month=10, start_day=17, end_year=2018, end_month=4, end_day=12,\n",
    "                             season='2017-18', season_start='10/17/2017')\n",
    "            df = make_dataframe(all_games)\n",
    "\n",
    "            print(df)\n",
    "            df.to_csv(r'C:\\Users\\alvaro\\OneDrive\\Documents\\School\\Flatiron\\Projects\\NBA_Prediction_Model\\data\\nba_df_2017.csv', index=False)\n",
    "        except requests.exceptions.ReadTimeout:\n",
    "            if i < attempts - 1:\n",
    "                continue\n",
    "            else:\n",
    "                raise\n",
    "        break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2017 - 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_2017 = pd.read_csv('./data/nba_df_2017.csv')\n",
    "df_2017['Date'] = pd.to_datetime(df_2017['Date'])\n",
    "df_2017['Season'] = '2017-18'\n",
    "\n",
    "print(len(df_2018))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2018 - 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018 = pd.read_csv('./data/nba_df_2018.csv')\n",
    "df_2018['Date'] = pd.to_datetime(df_2018['Date'])\n",
    "df_2018['Season'] = '2018-19'\n",
    "\n",
    "print(len(df_2018))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2019 - 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2019 = pd.read_csv('./data/nba_df_2019.csv')\n",
    "df_2019['Date'] = pd.to_datetime(df_2019['Date'])\n",
    "df_2019['Season'] = '2019-20'\n",
    "\n",
    "df_2019_2 = pd.read_csv('./data/nba_df_2019_2.csv')\n",
    "df_2019_2['Date'] = pd.to_datetime(df_2019_2['Date'])\n",
    "df_2019_2['Season'] = '2019-20'\n",
    "\n",
    "print(len(df_2019), len(df_2019_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_2019, df_2019_2]\n",
    "df_2019_final = pd.concat(frames)\n",
    "len(df_2019_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2020 - 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2021 = pd.read_csv('./data/nba_df_2020_v0.csv')\n",
    "df_2021['Date'] = pd.to_datetime(df_2021['Date'])\n",
    "df_2021['Season'] = '2020-21'\n",
    "\n",
    "df_2021_2 = pd.read_csv('./data/nba_df_2020.csv')\n",
    "df_2021_2['Date'] = pd.to_datetime(df_2021_2['Date'])\n",
    "df_2021_2['Season'] = '2020-21'\n",
    "\n",
    "df_2021_3 = pd.read_csv('./data/nba_df_2020_v2.csv')\n",
    "df_2021_3['Date'] = pd.to_datetime(df_2021_3['Date'])\n",
    "df_2021_3['Season'] = '2020-21'\n",
    "\n",
    "print(len(df_2021), len(df_2021_2), len(df_2021_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_2021, df_2021_2, df_2021_3]\n",
    "df_2021_final = pd.concat(frames)\n",
    "len(df_2021_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2021 - 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_2022 = pd.read_csv('./data/nba_game_2022.csv')\n",
    "df_2022['Date'] = pd.to_datetime(df_2022['Date'])\n",
    "df_2022['Season'] = '2021-22'\n",
    "\n",
    "df_2022_1 = pd.read_csv('./data/nba_game_2022_v1.csv')\n",
    "df_2022_1['Date'] = pd.to_datetime(df_2022_1['Date'])\n",
    "df_2022_1['Season'] = '2021-22'\n",
    "\n",
    "df_2022_2 = pd.read_csv('./data/nba_game_2022_v2.csv')\n",
    "df_2022_2['Date'] = pd.to_datetime(df_2022_2['Date'])\n",
    "df_2022_2['Season'] = '2021-22'\n",
    "\n",
    "df_2022_3 = pd.read_csv('./data/nba_game_2022_v3.csv')\n",
    "df_2022_3['Date'] = pd.to_datetime(df_2022_3['Date'])\n",
    "df_2022_3['Season'] = '2021-22'\n",
    "\n",
    "print(len(df_2022), len(df_2022_1), len(df_2022_2), len(df_2022_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [df_2022, df_2022_1, df_2022_2, df_2022_3]\n",
    "df_2022_final = pd.concat(frames)\n",
    "\n",
    "print(f\"Length of 2022 data: {len(df_2022_final)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frames = [df_2018, df_2019_final, df_2021_final, df_2022_final]\n",
    "df = pd.concat(frames)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df[df.duplicated()]\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last N Games Win %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "prev_game_df = df[df['Date'] < '12/25/2022'][(df['Home'] == \"Golden State Warriors\") | (df['Away'] == 'Golden State Warriors')].sort_values(by = 'Date').tail(10)\n",
    "prev_game_df\n",
    "h_df = prev_game_df.iloc[:, range(0, 32, 31)]\n",
    "\n",
    "h_df = h_df.loc[h_df['Home'] == 'Golden State Warriors'] \n",
    "print(h_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_win_pct_last_n_games(team, game_date, df, n):\n",
    "    prev_game_df = df[df['Date'] < game_date][(df['Home'] == team) | (df['Away'] == team)].sort_values(by = 'Date').tail(n)\n",
    "    \n",
    "    wins = 0 \n",
    "    \n",
    "    result_df = prev_game_df.iloc[:, range(0,32,31)]\n",
    "    h_df = result_df.loc[result_df['Home'] == team] \n",
    "    \n",
    "    h_wins = h_df.loc[h_df['Result'] == 1]\n",
    "    \n",
    "    wins += len(h_wins)\n",
    "      \n",
    "    a_df = result_df.loc[result_df['Home'] != team]\n",
    "    a_wins = a_df.loc[a_df['Result'] == 0]\n",
    "    \n",
    "    wins += len(a_wins)\n",
    "\n",
    "    return wins/n\n",
    "get_avg_win_pct_last_n_games('Golden State Warriors', '12/25/2022', df, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for season in df['Season'].unique() :\n",
    "    \n",
    "    season_stats = df[df['Season'] == season].sort_values(by='Date').reset_index(drop=True)\n",
    "    \n",
    "    for index, row in df.iterrows() : \n",
    "        game_id = row['Game_ID']\n",
    "        game_date = row['Date']\n",
    "        h_team = row['Home']\n",
    "        a_team = row['Away']\n",
    "        \n",
    "        df.loc[index,'Home_W_Pct_10'] = get_avg_win_pct_last_n_games(h_team, game_date, df, 10)\n",
    "        \n",
    "        df.loc[index,'Away_W_Pct_10'] = get_avg_win_pct_last_n_games(a_team, game_date, df, 10)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['Season'] == '2021-22'].tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELO Rating\n",
    "- every team starts with a 1500\n",
    "$$R_{i+1} = k * (S_{team} - E_{team} + R_{i})$$\n",
    "- S team is 1 if the team wins and 0 if they lose\n",
    "- E team is the expected win probability of the team \n",
    "$$E_{team} = \\frac{1}{1+10^{\\frac{opp\\_elo - team\\_elo}{400}}}$$\n",
    "- k is a moving constant that depends on margin of victory and difference in Elo ratings\n",
    "$$k = 20\\frac{(MOV_{winner} + 3)^{0.8}}{7.5 + 0.006(elo\\_difference_{winner})} $$\n",
    "- team year by year carryover \n",
    "$$(R * 0.75) + (0.25 * 1505)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Home and road team win probabilities implied by Elo ratings and home court adjustment \n",
    "import math\n",
    "import time\n",
    "def win_probs(home_elo, away_elo, home_court_advantage) :\n",
    "    h = math.pow(10, home_elo/400)\n",
    "    r = math.pow(10, away_elo/400)\n",
    "    a = math.pow(10, home_court_advantage/400) \n",
    "\n",
    "    denom = r + a*h\n",
    "    home_prob = a*h / denom\n",
    "    away_prob = r / denom \n",
    "  \n",
    "    return home_prob, away_prob\n",
    "\n",
    "  #odds the home team will win based on elo ratings and home court advantage\n",
    "\n",
    "def home_odds_on(home_elo, away_elo, home_court_advantage) :\n",
    "    h = math.pow(10, home_elo/400)\n",
    "    r = math.pow(10, away_elo/400)\n",
    "    a = math.pow(10, home_court_advantage/400)\n",
    "    return a*h/r\n",
    "\n",
    "#this function determines the constant used in the elo rating, based on margin of victory and difference in elo ratings\n",
    "def elo_k(MOV, elo_diff):\n",
    "    k = 20 # Optimal K is 20 https://fivethirtyeight.com/features/how-we-calculate-nba-elo-ratings/\n",
    "    if MOV>0:\n",
    "        multiplier=(MOV+3)**(0.8)/(7.5+0.006*(elo_diff))\n",
    "    else:\n",
    "        multiplier=(-MOV+3)**(0.8)/(7.5+0.006*(-elo_diff))\n",
    "    return k*multiplier\n",
    "\n",
    "\n",
    "# Updates the home and away teams elo ratings after a game \n",
    "\n",
    "def update_elo(home_score, away_score, home_elo, away_elo, home_court_advantage) :\n",
    "    home_prob, away_prob = win_probs(home_elo, away_elo, home_court_advantage) \n",
    "\n",
    "    if (home_score - away_score > 0) :\n",
    "        home_win = 1 \n",
    "        away_win = 0 \n",
    "    else :\n",
    "        home_win = 0 \n",
    "        away_win = 1 \n",
    "  \n",
    "    k = elo_k(home_score - away_score, home_elo - away_elo)\n",
    "\n",
    "    updated_home_elo = home_elo + k * (home_win - home_prob) \n",
    "    updated_away_elo = away_elo + k * (away_win - away_prob)\n",
    "    \n",
    "    return updated_home_elo, updated_away_elo\n",
    "\n",
    "\n",
    "# Takes into account prev season elo\n",
    "# The reason we revert to a mean of 1505 rather than 1500 is that \n",
    "# there are liable to be a couple of relatively recent expansion teams in the league at any given time\n",
    "def get_prev_elo(team, date, season, team_stats, elo_df) :\n",
    "    prev_game = team_stats[team_stats['Date'] < game_date][(team_stats['Home'] == team) | (team_stats['Away'] == team)].sort_values(by = 'Date').tail(1).iloc[0] \n",
    "\n",
    "    if team == prev_game['Home'] :\n",
    "        elo_rating = elo_df[elo_df['Game_ID'] == prev_game['Game_ID']]['H_Team_Elo_After'].values[0]\n",
    "    else :\n",
    "        elo_rating = elo_df[elo_df['Game_ID'] == prev_game['Game_ID']]['A_Team_Elo_After'].values[0]\n",
    "  \n",
    "    if prev_game['Season'] != season :\n",
    "        return (0.75 * elo_rating) + (0.25 * 1505) # Year-to-Year Carry-Over\n",
    "    else :\n",
    "        return elo_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by = 'Date', inplace = True)\n",
    "df.reset_index(inplace=True, drop = True)\n",
    "elo_df = pd.DataFrame(columns=['Game_ID', 'H_Team', 'A_Team', 'H_Team_Elo_Before', 'A_Team_Elo_Before', 'H_Team_Elo_After', 'A_Team_Elo_After'])\n",
    "teams_elo_df = pd.DataFrame(columns=['Game_ID','Team', 'Elo', 'Date', 'Where_Played', 'Season']) \n",
    "\n",
    "for index, row in df.iterrows(): \n",
    "    game_id = row['Game_ID']\n",
    "    game_date = row['Date']\n",
    "    season = row['Season']\n",
    "    h_team, a_team = row['Home'], row['Away']\n",
    "    h_score, a_score = row['H_Score'], row['A_Score'] \n",
    "\n",
    "    if (h_team not in elo_df['H_Team'].values and h_team not in elo_df['A_Team'].values) :\n",
    "        h_team_elo_before = 1500\n",
    "    else :\n",
    "        h_team_elo_before = get_prev_elo(h_team, game_date, season, df, elo_df)\n",
    "\n",
    "    if (a_team not in elo_df['H_Team'].values and a_team not in elo_df['A_Team'].values) :\n",
    "        a_team_elo_before = 1500\n",
    "    else :\n",
    "        a_team_elo_before = get_prev_elo(a_team, game_date, season, df, elo_df)\n",
    "\n",
    "    h_team_elo_after, a_team_elo_after = update_elo(h_score, a_score, h_team_elo_before, a_team_elo_before, 69)\n",
    "\n",
    "    new_row = {'Game_ID': game_id, 'H_Team': h_team, 'A_Team': a_team, 'H_Team_Elo_Before': h_team_elo_before, 'A_Team_Elo_Before': a_team_elo_before, \\\n",
    "                                                                        'H_Team_Elo_After' : h_team_elo_after, 'A_Team_Elo_After': a_team_elo_after}\n",
    "    teams_row_one = {'Game_ID': game_id,'Team': h_team, 'Elo': h_team_elo_before, 'Date': game_date, 'Where_Played': 'Home', 'Season': season}\n",
    "    teams_row_two = {'Game_ID': game_id,'Team': a_team, 'Elo': a_team_elo_before, 'Date': game_date, 'Where_Played': 'Away', 'Season': season}\n",
    "  \n",
    "    elo_df = elo_df.append(new_row, ignore_index = True)\n",
    "    teams_elo_df = teams_elo_df.append(teams_row_one, ignore_index=True)\n",
    "    teams_elo_df = teams_elo_df.append(teams_row_two, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#teams_elo_df.set_index([\"Team\"], append=True)\n",
    "#dataset = teams_elo_df.pivot(index=\"Team\",values=\"Elo\", columns=\"Date\")\n",
    "dates = list(set([d.strftime(\"%m-%d-%Y\") for d in teams_elo_df[\"Date\"]]))\n",
    "dates = sorted(dates, key=lambda x: time.strptime(x, '%m-%d-%Y'))\n",
    "teams = df[\"Away\"]\n",
    "dataset = pd.DataFrame(columns=dates)\n",
    "dataset[\"Team\"] = teams.drop_duplicates()\n",
    "dataset = dataset.set_index(\"Team\")\n",
    "\n",
    "for index, row in teams_elo_df.iterrows():\n",
    "    date = row[\"Date\"].strftime(\"%m-%d-%Y\")\n",
    "    team = row[\"Team\"]\n",
    "    elo = row[\"Elo\"]\n",
    "    dataset[date][team] = elo\n",
    "\n",
    "teams_elo_df['Elo'] = teams_elo_df['Elo'].astype(float)\n",
    "\n",
    "elo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(elo_df.drop(columns=['H_Team', 'A_Team']), on ='Game_ID')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization and Z Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Different Models - No Z Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'C:/Users/alvaro/OneDrive/Documents/School/Flatiron/Projects/NBA_Prediction_Model/data/nba_raw.csv', index=False)\n",
    "print(f'The final dataset consists of three seasons and {len(df)} games.')\n",
    "df = df.reset_index(drop=True)\n",
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "df = df.drop(labels=['H_Team_Elo_After', 'A_Team_Elo_After'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"H_Team_Elo_Before\"] = df.H_Team_Elo_Before.astype(float)\n",
    "df[\"A_Team_Elo_Before\"] = df.A_Team_Elo_Before.astype(float)\n",
    "df = df.drop(['Home', 'Away', 'Game_ID', 'H_Score', 'A_Score', 'Date', 'Season'], axis=1)\n",
    "#df.head()\n",
    "#df.columns\n",
    "#df.info()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'C:\\Users\\alvaro\\OneDrive\\Documents\\School\\Flatiron\\Projects\\NBA_Prediction_Model\\data\\nba.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/nba.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(44, 34))\n",
    "correlation = df[['H_W_PCT', \n",
    "                  'H_REB', \n",
    "                  'H_AST',\n",
    "                  'H_TOV', \n",
    "                  'H_STL', \n",
    "                  'H_BLK', \n",
    "                  'H_PLUS_MINUS', \n",
    "                  'H_OFF_RATING',\n",
    "                  'H_DEF_RATING', \n",
    "                  'H_TS_PCT', \n",
    "                  'H_Team_Elo_Before', \n",
    "                  'Home_W_Pct_10', \n",
    "                  'Result'\n",
    "                  ]]\n",
    "\n",
    "sns.heatmap(correlation.corr(), annot=True);\n",
    "# correlation\n",
    "# sns.heatmap(df.corr(), annot=True);\n",
    "# sns.heatmap(df['Result'].corr(), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.corr()['Result'].abs().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    fig, ax = plt.subplots(figsize=(44, 34))\n",
    "    \n",
    "    mask = np.triu(np.ones_like(df.corr(), dtype=bool))\n",
    "    ax = sns.heatmap(abs(df.corr()),mask=mask,annot=True)\n",
    "    fig.savefig('images/Corelation_Heatmap');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#import final dataset\n",
    "#final_df = pd.read_csv('')\n",
    "\n",
    "#drop non numeric columns\n",
    "#df.drop(columns = ['Home', 'Away', 'Game_ID', 'Date', 'Season'], axis = 1, inplace = True )\n",
    "\n",
    "X = final_df.drop(columns = 'Result')\n",
    "\n",
    "y = final_df['Result']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print(f'X train shape: {X_train.shape}')\n",
    "print(f'X test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"Raw Counts \n",
    "{final_df[\"Result\"].value_counts()}\\n\n",
    "Percentages \n",
    "{final_df[\"Result\"].value_counts(normalize=True)}\n",
    "\n",
    "\n",
    "We would get an accuracy score of 0.566312 with a baseline model, i.e. about 56.6% accuracy\n",
    "\n",
    "This is because about 56.6% of the results are wins\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"Train percent wins\n",
    "{y_train.value_counts(normalize=True)}\\n\"\"\")\n",
    "\n",
    "print(f\"\"\"Test percent wins: \n",
    "{y_test.value_counts(normalize=True)}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "class ModelWithCV():\n",
    "    '''Structure to save the model and more easily see its crossvalidation'''\n",
    "    \n",
    "    def __init__(self, model, model_name, X, y, cv_now=True):\n",
    "        self.model = model\n",
    "        self.name = model_name\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        # For CV results\n",
    "        self.cv_results = None\n",
    "        self.cv_mean = None\n",
    "        self.cv_median = None\n",
    "        self.cv_std = None\n",
    "        #\n",
    "        if cv_now:\n",
    "            self.cross_validate()\n",
    "        \n",
    "    def cross_validate(self, X=None, y=None, kfolds=10):\n",
    "        '''\n",
    "        Perform cross-validation and return results.\n",
    "        \n",
    "        Args: \n",
    "          X:\n",
    "            Optional; Training data to perform CV on. Otherwise use X from object\n",
    "          y:\n",
    "            Optional; Training data to perform CV on. Otherwise use y from object\n",
    "          kfolds:\n",
    "            Optional; Number of folds for CV (default is 10)  \n",
    "        '''\n",
    "        \n",
    "        cv_X = X if X else self.X\n",
    "        cv_y = y if y else self.y\n",
    "\n",
    "        self.cv_results = cross_val_score(self.model, cv_X, cv_y, cv=kfolds)\n",
    "        self.cv_mean = np.mean(self.cv_results)\n",
    "        self.cv_median = np.median(self.cv_results)\n",
    "        self.cv_std = np.std(self.cv_results)\n",
    "\n",
    "        \n",
    "    def print_cv_summary(self):\n",
    "        cv_summary = (\n",
    "        f'''CV Results for `{self.name}` model:\n",
    "            {self.cv_mean:.5f} Â± {self.cv_std:.5f} accuracy\n",
    "        ''')\n",
    "        print(cv_summary)\n",
    "\n",
    "        \n",
    "    def plot_cv(self, ax):\n",
    "        '''\n",
    "        Plot the cross-validation values using the array of results and given \n",
    "        Axis for plotting.\n",
    "        '''\n",
    "        ax.set_title(f'CV Results for `{self.name}` Model')\n",
    "        # Thinner violinplot with higher bw\n",
    "        sns.violinplot(y=self.cv_results, ax=ax, bw=.4)\n",
    "        sns.swarmplot(\n",
    "                y=self.cv_results,\n",
    "                color='orange',\n",
    "                size=10,\n",
    "                alpha= 0.8,\n",
    "                ax=ax\n",
    "        )\n",
    "\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dummy/Baseliner\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "estimator = DummyRegressor(strategy='mean')\n",
    "\n",
    "# Create Dummy/Baseliner\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('estimator', DummyRegressor(strategy='mean'))\n",
    "])\n",
    "\n",
    "cv = ModelWithCV(pipe, 'estimator', X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "\n",
    "cv.plot_cv(ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.print_cv_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_pipe.print_cv_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant class and function\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    #('scaler', StandardScaler()),\n",
    "    #('pca', PCA(n_components=4)),\n",
    "    ('estimator', LogisticRegression(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {}\n",
    "param_grid['estimator__C'] = np.logspace(-4, 4, 50)\n",
    "param_grid['estimator__solver'] = ['liblinear']\n",
    "param_grid['estimator__penalty'] = ['l1', 'l2']\n",
    "param_grid['estimator__class_weight'] = ['balanced', None]\n",
    "#params['logreg__n_jobs'] = [1]\n",
    "param_grid['estimator__max_iter'] = [1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv=10, return_train_score=True, scoring='accuracy', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest - Random Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('estimator', RandomForestClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {}\n",
    "param_grid['estimator__n_estimators'] = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "param_grid['estimator__max_features'] = ['auto', 'sqrt']\n",
    "param_grid['estimator__max_depth'] = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "param_grid['estimator__min_samples_split'] = [2, 5, 10]\n",
    "param_grid['estimator__min_samples_leaf'] = [1, 2, 4]\n",
    "param_grid['estimator__bootstrap'] = [True, False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "grid_search = RandomizedSearchCV(estimator=pipe, \n",
    "                                 param_distributions=param_grid, \n",
    "                                 cv=10, return_train_score=True, \n",
    "                                 scoring='accuracy', \n",
    "                                 n_iter=100, \n",
    "                                 random_state=42, \n",
    "                                 n_jobs = -1, \n",
    "                                 verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest - Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('estimator', RandomForestClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {}\n",
    "param_grid['estimator__n_estimators'] = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
    "param_grid['estimator__max_features'] = ['auto', 'sqrt']\n",
    "param_grid['estimator__max_depth'] = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "param_grid['estimator__min_samples_split'] = [2, 5, 10]\n",
    "param_grid['estimator__min_samples_leaf'] = [1, 2, 4]\n",
    "param_grid['estimator__bootstrap'] = [True, False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipe, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=10, \n",
    "                           return_train_score=True, \n",
    "                           scoring='accuracy',\n",
    "                           n_jobs = -1,\n",
    "                           verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('estimator', GaussianNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {}\n",
    "param_grid['estimator__var_smoothing'] = np.logspace(0,-9, num=100)\n",
    "#param_grid['estimator__var_smoothing'] = [1e-11, 1e-10, 1e-9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "grid_search = RandomizedSearchCV(estimator=pipe, \n",
    "                                 param_distributions=param_grid, \n",
    "                                 cv=10, \n",
    "                                 return_train_score=True, \n",
    "                                 scoring='accuracy', \n",
    "                                 random_state=42,\n",
    "                                 n_jobs = -1,\n",
    "                                 verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('estimator', xgboost.XGBRegressor(random_state=42, objective='reg:squarederror'))\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {}\n",
    "param_grid['estimator__min_child_weight'] = [1, 5, 10],\n",
    "param_grid['estimator__gamma'] = [0.5, 1, 1.5, 2, 5],\n",
    "param_grid['estimator__subsample'] = [0.6, 0.8, 1.0],\n",
    "param_grid['estimator__colsample_bytree'] = [0.6, 0.8, 1.0],\n",
    "param_grid['estimator__max_depth'] = [3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipe, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=10, \n",
    "                           return_train_score=True, \n",
    "                           scoring='accuracy',\n",
    "                           n_jobs = -1,\n",
    "                           verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#script to test the effectivenes of each model, uses default parameters\n",
    "#test six different classification models \n",
    "def run_exps(X_train, y_train, X_test, y_test) :\n",
    "    '''\n",
    "    Lightweight script to test many models and find winners\n",
    "    :param X_train: training split\n",
    "    :param y_train: training target vector\n",
    "    :param X_test: test split\n",
    "    :param y_test: test target vector\n",
    "    :return: DataFrame of predictions\n",
    "    '''\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    models = [\n",
    "          ('LogReg', LogisticRegression()), \n",
    "          ('RF', RandomForestClassifier()),\n",
    "          ('KNN', KNeighborsClassifier()),\n",
    "          ('SVM', SVC()), \n",
    "          ('GNB', GaussianNB()),\n",
    "          ('XGB', XGBClassifier())\n",
    "        ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    names = []\n",
    "    \n",
    "    scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted', 'roc_auc']\n",
    "    \n",
    "    target_names = ['win', 'loss']\n",
    "    \n",
    "    for name, model in models:\n",
    "        \n",
    "        kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=90210)\n",
    "        cv_results = model_selection.cross_validate(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "        clf = model.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        print(name)\n",
    "        print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "        \n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        \n",
    "        this_df = pd.DataFrame(cv_results)\n",
    "        this_df['model'] = name\n",
    "        dfs.append(this_df)\n",
    "        \n",
    "    final = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    return final\n",
    "final = run_exps(X_train, y_train, X_test, y_test)\n",
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstraps = []\n",
    "for model in list(set(final.model.values)):\n",
    "    model_df = final.loc[final.model == model]\n",
    "    bootstrap = model_df.sample(n=30, replace=True)\n",
    "    bootstraps.append(bootstrap)\n",
    "        \n",
    "bootstrap_df = pd.concat(bootstraps, ignore_index=True)\n",
    "results_long = pd.melt(bootstrap_df,id_vars=['model'],var_name='metrics', value_name='values')\n",
    "time_metrics = ['fit_time','score_time'] # fit time metrics\n",
    "## PERFORMANCE METRICS\n",
    "results_long_nofit = results_long.loc[~results_long['metrics'].isin(time_metrics)] # get df without fit data\n",
    "results_long_nofit = results_long_nofit.sort_values(by='values')\n",
    "## TIME METRICS\n",
    "results_long_fit = results_long.loc[results_long['metrics'].isin(time_metrics)] # df with fit data\n",
    "results_long_fit = results_long_fit.sort_values(by='values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(20, 12))\n",
    "sns.set(font_scale=2.5)\n",
    "g = sns.boxplot(x=\"model\", y=\"values\", hue=\"metrics\", data=results_long_nofit, palette=\"Set3\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('Comparison of Model by Classification Metric')\n",
    "plt.savefig('./benchmark_models_performance.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "sns.set(font_scale=2.5)\n",
    "g = sns.boxplot(x=\"model\", y=\"values\", hue=\"metrics\", data=results_long_fit, palette=\"Set3\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('Comparison of Model by Fit and Score Time')\n",
    "plt.savefig('./benchmark_models_time.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = list(set(results_long_nofit.metrics.values))\n",
    "bootstrap_df.groupby(['model'])[metrics].agg([np.std, np.mean])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Selected Model\n",
    "- grid search for parameters \n",
    "- Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian NB only has one parameter 'var_smoothing'\n",
    "# Portion of the largest variance of all features that is added to variances for calculation stability.\n",
    "# Number of different combinations of parameters \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "nb_classifier = GaussianNB()\n",
    "\n",
    "target_names = ['Win', 'Loss']\n",
    "\n",
    "params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=90210)\n",
    "\n",
    "gs_NB = GridSearchCV(estimator=nb_classifier, \n",
    "                 param_grid=params_NB, \n",
    "                 cv=kfold,   \n",
    "                 verbose=1, \n",
    "                 scoring='accuracy', n_jobs=-1) \n",
    "\n",
    "gs_NB.fit(X_train, y_train)\n",
    "\n",
    "best_gs_grid = gs_NB.best_estimator_\n",
    "best_gs_grid.fit(X_train, y_train)\n",
    "y_pred_best_gs = best_gs_grid.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_best_gs, target_names=target_names))\n",
    "gs_NB.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusionMatrix = confusion_matrix(y_test, y_pred_best_gs)\n",
    "print(confusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionMatrix = confusion_matrix(y_test, y_pred_best_gs)  \n",
    "\n",
    "    # Code below prints model accuracy information\n",
    "print('Coefficient Information:')\n",
    "\n",
    "for i in range(len(featureColumns)):  \n",
    "\n",
    "    logregCoefficients = logreg.coef_\n",
    "\n",
    "    currentFeature = featureColumns[i]\n",
    "    currentCoefficient = logregCoefficients[0][i]\n",
    "\n",
    "    print(currentFeature + ': ' + str(currentCoefficient))\n",
    "\n",
    "print('----------------------------------')\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(Y_test, Y_pred))\n",
    "print(\"Precision:\", metrics.precision_score(Y_test, Y_pred))\n",
    "print(\"Recall:\", metrics.recall_score(Y_test, Y_pred))\n",
    "\n",
    "print('----------------------------------')\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(confusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Saves the model in folder to be used in future\n",
    "# filename should be end in '.pkl'\n",
    "def save_model(model, filename):\n",
    "\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
