{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what we're working with\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import requests\n",
    "import nba_api\n",
    "from nba_api.stats.endpoints import teamdashboardbygeneralsplits, leaguedashteamstats\n",
    "from nba_api.stats.endpoints import leaguegamelog, scoreboard, leaguestandings\n",
    "\n",
    "from datetime import date, timedelta\n",
    "import time\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# from sklearn import model_selection\n",
    "# from sklearn.utils import class_weight\n",
    "# from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile a dictionary of all 30 NBA teams and their Team ID\n",
    "teams = {\"Atlanta Hawks\": 1610612737,\n",
    "        \"Boston Celtics\": 1610612738,\n",
    "        \"Brooklyn Nets\": 1610612751,\n",
    "        \"Charlotte Bobcats\": 1610612766,\n",
    "        \"Charlotte Hornets\": 1610612766,\n",
    "        \"Chicago Bulls\": 1610612741,\n",
    "        \"Cleveland Cavaliers\": 1610612739,\n",
    "        \"Dallas Mavericks\": 1610612742,\n",
    "        \"Denver Nuggets\": 1610612743,\n",
    "        \"Detroit Pistons\": 1610612765,\n",
    "        \"Golden State Warriors\": 1610612744,\n",
    "        \"Houston Rockets\": 1610612745,\n",
    "        \"Indiana Pacers\": 1610612754,\n",
    "        \"LA Clippers\": 1610612746,\n",
    "        \"Los Angeles Clippers\": 1610612746,\n",
    "        \"Los Angeles Lakers\": 1610612747,\n",
    "        \"Memphis Grizzlies\": 1610612763,\n",
    "        \"Miami Heat\": 1610612748,\n",
    "        \"Milwaukee Bucks\": 1610612749,\n",
    "        \"Minnesota Timberwolves\": 1610612750,\n",
    "        \"New Jersey Nets\": 1610612751,\n",
    "        \"New Orleans Hornets\": 1610612740,\n",
    "        \"New Orleans Pelicans\": 1610612740,\n",
    "        \"New York Knicks\": 1610612752,\n",
    "        \"Oklahoma City Thunder\": 1610612760,\n",
    "        \"Orlando Magic\": 1610612753,\n",
    "        \"Philadelphia 76ers\": 1610612755,\n",
    "        \"Phoenix Suns\": 1610612756,\n",
    "        \"Portland Trail Blazers\": 1610612757,\n",
    "        \"Sacramento Kings\": 1610612758,\n",
    "        \"San Antonio Spurs\": 1610612759,\n",
    "        \"Toronto Raptors\": 1610612761,\n",
    "        \"Utah Jazz\": 1610612762,\n",
    "        \"Washington Wizards\": 1610612764,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Stats\n",
    "Stats like field goal percentage, rebounds, and turnovers are easily digested by NBA viewers. Some people don't like stats. They would rather apply the eye-test and see for themselves whether a team is any good. Others realize that stats can tell a story about the game but only if they know how to use them. Advanced stats play this role and help us dissect the drama unfolding on the court. Therefore, traditional and advanced stats will be cast in our models.\n",
    "\n",
    "### True Shooting Percentage\n",
    "There are 3 ways that an NBA player can score: 3-pointers, 2-pointers and free throws. True shooting percentage ('TS_PCT') looks at all three. 3-pointers are a little tricky to factor into the equation. The max true shooting percentage  is 150% and can only be reached if a player hits every one of their shots and they're all from behind the arch. Because this stat accounts for all shots, it's easily the best measure of shooting ability. \n",
    "\n",
    "For example, if a player goes 1-for-1 and their only shot is from the hash-mark, the formula will read and simplify as follows (please just trust and accept that the .44 multiplier is the best way of estimating the total number of possessions a player is involved in):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{points} {2 *fga + .44 * fta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{3}  {2 * 1 + .44 * 0} = \\frac{3}{2} = {1.5}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile a dictionary of stats and their source \n",
    "available_stats = {'W_PCT': 'Base',\n",
    "                   'FG_PCT': 'Base',\n",
    "                   'FG3_PCT': 'Base',\n",
    "                   'FT_PCT': 'Base',\n",
    "                   'REB': 'Base',\n",
    "                   'AST': 'Base',\n",
    "                   'TOV': 'Base',\n",
    "                   'STL': 'Base',\n",
    "                   'BLK': 'Base',\n",
    "                   'PLUS_MINUS': 'Base',\n",
    "                   'OFF_RATING': 'Advanced',\n",
    "                   'DEF_RATING': 'Advanced',\n",
    "                   'TS_PCT': 'Advanced'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offensive and defensive rating\n",
    "Basketball stresses efficiency. Minimizing points allowed and maximizing points scored on each possession is more important than overall totals. Totals are shaped by variables like pace — or the number of possessions a team gets in a game — which differs depending on coaching (i.e. the Golden State Warriors averaged 3 fewer possessions per game than the Los Angeles Lakers last season).\n",
    "\n",
    "This is where tempo-free stats offensive and defensive rating come into play. Defensive rating shows how many points a player allows per 100 possessions. This statistic functions differently than a plus/minus system, where all points scored while a player is on the court count against them. Only the shots that are scored as a result of their defensive lapses are counted against them. \n",
    "\n",
    "Offensive rating is simpler to calculate. It's just the amount of points produced by a player per 100 possessions. Again, the reason offensive and defensive ratings are useful is because they're tempo-free stats. Offensive and defensive rating eliminate factors like pace of play and minutes played per game. Below is the formula for offensive rating:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{100*pp} {fga + .44 * fta + to}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nba_api.stats.endpoints import teamdashboardbygeneralsplits, leaguedashteamstats\n",
    "\n",
    "\n",
    "def get_team_stats_dict(team, start_date, end_date, season='2021-22'):\n",
    "    \"\"\"\n",
    "    Returns the stats for the selected team in a dataframe, default year is 2021-22\n",
    "    :param start_data: Day of games scheduled in form 'mm/dd/yyyy'\n",
    "    :param end_data: Day of games scheduled in form 'mm/dd/yyyy'\n",
    "    :param season: Day of games scheduled in form 'yyyy-yy'\n",
    "    :return: A dictionary of game matchups {home_team:[away_team]}\n",
    "    \"\"\"\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    \n",
    "    # Load teamdashboardbygeneralsplits to access 'Per100Possessions' team stats\n",
    "    general_team_info = teamdashboardbygeneralsplits.TeamDashboardByGeneralSplits(team_id=teams[team],\n",
    "                                                                                  per_mode_detailed='Per100Possessions',\n",
    "                                                                                  season=season,\n",
    "                                                                                  date_from_nullable=start_date,\n",
    "                                                                                  date_to_nullable=end_date,\n",
    "                                                                                  timeout=120)\n",
    "    # Move into general_team_info dictionary\n",
    "    general_team_dict = general_team_info.get_normalized_dict()\n",
    "    general_team_dashboard = general_team_dict['OverallTeamDashboard'][0]\n",
    "\n",
    "\n",
    "    # Select stat columns to webscrape from general_team_info dictionary\n",
    "    win_percentage = general_team_dashboard['W_PCT']\n",
    "    fg_percentage = general_team_dashboard['FG_PCT']\n",
    "    fg3_percentage = general_team_dashboard['FG3_PCT']\n",
    "    ft_percentage = general_team_dashboard['FT_PCT']\n",
    "    rebounds = general_team_dashboard['REB']\n",
    "    assists = general_team_dashboard['AST']\n",
    "    turnovers = general_team_dashboard['TOV']\n",
    "    steals = general_team_dashboard['STL']\n",
    "    blocks = general_team_dashboard['BLK']\n",
    "    plus_minus = general_team_dashboard['PLUS_MINUS']\n",
    "\n",
    "    \n",
    "    # Load teamdashboardbygeneralsplits to access Advanced team stats\n",
    "    advanced_team_info = teamdashboardbygeneralsplits.TeamDashboardByGeneralSplits(team_id=teams[team],\n",
    "                                                                                   measure_type_detailed_defense='Advanced',\n",
    "                                                                                   season=season,\n",
    "                                                                                   date_from_nullable=start_date,\n",
    "                                                                                   date_to_nullable=end_date,\n",
    "                                                                                   timeout=120)\n",
    "                                                                            \n",
    "    # Move into advanced_team_info dictionary\n",
    "    advanced_team_dict = advanced_team_info.get_normalized_dict()\n",
    "    advanced_team_dashboard = advanced_team_dict['OverallTeamDashboard'][0]\n",
    "\n",
    "    # Select stat columns to webscrape from advanced_team_info dictionary\n",
    "    offensive_rating = advanced_team_dashboard['OFF_RATING']\n",
    "    defensive_rating = advanced_team_dashboard['DEF_RATING']\n",
    "    true_shooting_percentage = advanced_team_dashboard['TS_PCT']\n",
    "\n",
    "    # Create a dictionary containing both the traditional and advanced stats \n",
    "    # and match them with the correspondiing variables outline above\n",
    "    all_stats_dict = {'W_PCT': win_percentage, \n",
    "                      'FG_PCT': fg_percentage, \n",
    "                      'FG3_PCT': fg3_percentage, \n",
    "                      'FT_PCT': ft_percentage, \n",
    "                      'REB': rebounds, \n",
    "                      'AST': assists, \n",
    "                      'TOV': turnovers, \n",
    "                      'STL': steals, \n",
    "                      'BLK': blocks, \n",
    "                      'PLUS_MINUS': plus_minus, \n",
    "                      'OFF_RATING': offensive_rating, \n",
    "                      'DEF_RATING': defensive_rating, \n",
    "                      'TS_PCT': true_shooting_percentage}\n",
    "\n",
    "    return all_stats_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call get_team_stats_dict() function and pass in a team, start_date, end_date, season\n",
    "get_team_stats_dict('Golden State Warriors', '10/19/2021', '04/10/2022', '2021-22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nba_api.stats.endpoints import leaguegamelog, scoreboard, leaguestandings\n",
    "\n",
    "def get_match_results(date, season):\n",
    "    \"\"\"\n",
    "    Returns the matchup and result of the game\n",
    "\n",
    "    :param date: Day of games scheduled in form 'mm/dd/yyyy'\n",
    "    :param season: Season in form of 'yyyy-yy'\n",
    "    :return: [{Golden State Warriors: Boston Celtics}], ['W']\n",
    "    \"\"\"\n",
    "\n",
    "    # Load leaguegamelog to access 'Regular Season' game logs\n",
    "    game_log = leaguegamelog.LeagueGameLog(season=season, \n",
    "                                           league_id='00', \n",
    "                                           date_from_nullable=date,\n",
    "                                           date_to_nullable=date, \n",
    "                                           season_type_all_star='Regular Season', \n",
    "                                           timeout=120)\n",
    "    # Move into game_log dictionary\n",
    "    game_log_dict = game_log.get_normalized_dict()\n",
    "    list_of_teams = game_log_dict['LeagueGameLog']\n",
    "\n",
    "    daily_match = {}\n",
    "    win_loss = []\n",
    "    score = []\n",
    "    game_id = []\n",
    "\n",
    "    for i in range(0, len(list_of_teams), 2):\n",
    "\n",
    "        if '@' in list_of_teams[i]['MATCHUP']:\n",
    "\n",
    "            # Select Away team\n",
    "            away_team = list_of_teams[i]['TEAM_NAME']\n",
    "            \n",
    "            # Select Home team\n",
    "            home_team = list_of_teams[i + 1]['TEAM_NAME']\n",
    "\n",
    "            # Append Home team win or loss\n",
    "            win_loss.append(list_of_teams[i + 1]['WL'])\n",
    "\n",
    "            # Append Game ID\n",
    "            game_id.append(list_of_teams[i + 1]['GAME_ID'])\n",
    "\n",
    "            # Append Home team score\n",
    "            score.append(list_of_teams[i + 1]['PTS'])\n",
    "            \n",
    "            # Append Away team score\n",
    "            score.append(list_of_teams[i]['PTS'])\n",
    "\n",
    "        else:\n",
    "            # Select Away team\n",
    "            away_team = list_of_teams[i + 1]['TEAM_NAME']\n",
    "            \n",
    "            # Select Home team\n",
    "            home_team = list_of_teams[i]['TEAM_NAME']\n",
    "\n",
    "            # Append Away team win or loss\n",
    "            win_loss.append(list_of_teams[i]['WL'])\n",
    "\n",
    "            # Append Game ID\n",
    "            game_id.append(list_of_teams[i]['GAME_ID'])\n",
    "\n",
    "            # Append Away team score\n",
    "            score.append(list_of_teams[i]['PTS'])\n",
    "            \n",
    "            # Append Home team score\n",
    "            score.append(list_of_teams[i + 1]['PTS'])\n",
    "\n",
    "        daily_match.update({home_team: away_team})\n",
    "\n",
    "    match_results = [daily_match, win_loss, score, game_id]\n",
    "\n",
    "    return match_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daily_matches(date):\n",
    "    \"\"\"\n",
    "    This function creates a dictionary of daily game matchups and their results.\n",
    "\n",
    "    :param date: Day of games scheduled in form 'mm/dd/yyyy'\n",
    "    :return: A dictionary of game matchups {home_team:away_team}\n",
    "    \"\"\"\n",
    "\n",
    "    # Load scoreboard to access each team's 'TEAM_ID' \n",
    "    daily_match = scoreboard.Scoreboard(league_id='00', game_date=date, timeout=120)\n",
    "    \n",
    "    # Move into daily_match dictionary\n",
    "    daily_match_dict = daily_match.get_normalized_dict()\n",
    "    games = daily_match_dict['GameHeader']\n",
    "\n",
    "    match = {}\n",
    "\n",
    "    # Loop through games\n",
    "    for game in games:\n",
    "\n",
    "        # Select 'HOME_TEAM_ID'\n",
    "        home_team_id = game['HOME_TEAM_ID']\n",
    "\n",
    "        # Assign home_team variable with the team name that goes along with the home_team_id\n",
    "        for team, team_id in teams.items():\n",
    "            if team_id == home_team_id:\n",
    "                home_team = team\n",
    "\n",
    "        # Select 'VISITOR_TEAM_ID'\n",
    "        away_team_id = game['VISITOR_TEAM_ID']\n",
    "\n",
    "        # Assign away_team variable with the team name that goes along with the away_team_id\n",
    "        for team, team_id in teams.items():\n",
    "            if team_id == away_team_id:\n",
    "                away_team = team\n",
    "\n",
    "        # Update the match dictionary with a dictionary of Home and Away team names\n",
    "        match.update({home_team: away_team})\n",
    "\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run both get_daily_matches() and get_match_results() functions\n",
    "def main():\n",
    "    print(f\"\"\"\n",
    "    'get_daily_matches()' returns a dictionary of the games on a specified date\\n{get_daily_matches('12/25/22')}\\n\n",
    "    \"\"\")\n",
    "    \n",
    "    print(f\"\"\"\n",
    "    'get_match_results()' returns the matchup plus the result\\n{get_match_results('10/19/2021', '2021-22')}\n",
    "    \"\"\")\n",
    "    \n",
    "# 1. Return a dictionary of the games on a specified date\n",
    "# 2. Return the matchup plus the game results\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "import requests\n",
    "\n",
    "def to_dataframe(daily_games, start_date, end_date, season):\n",
    "    \"\"\"\n",
    "    This function creates a DataFrame of daily game matchups and their results.\n",
    "\n",
    "    :param daily_games: get_match_results()\n",
    "    :param start_data: Day of games scheduled in form 'mm/dd/yyyy'\n",
    "    :param end_data: Day of games scheduled in form 'mm/dd/yyyy'\n",
    "    :param season: Day of games scheduled in form 'yyyy-yy'\n",
    "    :return: A dictionary of game matchups {home_team:away_team}\n",
    "    \"\"\"\n",
    "    full_dataframe = []\n",
    "    game_number = 0  # Counter to match with the corresponding game\n",
    "    daily_results = daily_games[1]  # Win or loss for each game\n",
    "    score = daily_games[2] # Score for the game\n",
    "    game_id = daily_games[3] # Game ID for the game\n",
    "\n",
    "    # loops through games to access home and away teams\n",
    "    for home_team, away_team in daily_games[0].items():  # loops through matchups\n",
    "        \n",
    "        # Pull home team stats\n",
    "        home_team_stats = get_team_stats_dict(home_team, start_date, end_date, season)\n",
    "        \n",
    "        # Pull away team stats\n",
    "        away_team_stats = get_team_stats_dict(away_team, start_date, end_date, season)\n",
    "\n",
    "        current_game = [home_team, away_team]\n",
    "        \n",
    "        current_game.append(game_id[game_number])\n",
    "\n",
    "        current_game.append(score.pop(0))\n",
    "\n",
    "        # Append home team stats\n",
    "        for stat, stat_type in available_stats.items():\n",
    "            current_game.append(home_team_stats[stat])\n",
    "        \n",
    "        current_game.append(score.pop(0))\n",
    "\n",
    "        # Append away team stats\n",
    "        for stat, stat_type in available_stats.items():\n",
    "            current_game.append(away_team_stats[stat])\n",
    "\n",
    "        # Assign 1 for a W and 0 for an L\n",
    "        if daily_results[game_number] == 'W':\n",
    "            result = 1\n",
    "        else:\n",
    "            result = 0\n",
    "\n",
    "        current_game.append(result)\n",
    "        game_number += 1\n",
    "\n",
    "        print(current_game)\n",
    "\n",
    "        # Appned full game stats to full_dataframe list\n",
    "        full_dataframe.append(current_game)\n",
    "\n",
    "    return full_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function  to plug in date ranges for game data you'd like to webscrape\n",
    "def date_range(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "        \n",
    "        \n",
    "# Define function  to plug in date ranges for game data you'd like to webscrape\n",
    "def training_set(start_year, start_month, start_day, end_year, end_month, end_day, season, season_start):\n",
    "    start_date = date(start_year, start_month, start_day)\n",
    "    end_date = date(end_year, end_month, end_day)\n",
    "\n",
    "    total_games = []\n",
    "\n",
    "    for single_date in date_range(start_date, end_date):\n",
    "        current_date = single_date.strftime('%m/%d/%Y')\n",
    "        print(current_date)\n",
    "\n",
    "        previous_day = single_date - timedelta(days=1)\n",
    "        previous_day_formatted = previous_day.strftime('%m/%d/%Y')\n",
    "\n",
    "        current_day_games = get_match_results(current_date, season)\n",
    "        current_day_games_with_stats = to_dataframe(current_day_games, season_start, previous_day_formatted, season)\n",
    "\n",
    "        for game in current_day_games_with_stats:\n",
    "            game.append(current_date)\n",
    "            total_games.append(game)\n",
    "\n",
    "    print(total_games)\n",
    "    return total_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame shell with column names to fill with webescraped game data\n",
    "def make_dataframe(game_list):\n",
    "    games = pd.DataFrame(game_list,\n",
    "                         columns=['Home', \n",
    "                                  'Away', \n",
    "                                  'Game_ID', \n",
    "                                  'H_Score', \n",
    "                                  'H_W_PCT', \n",
    "                                  'H_FG_PCT', \n",
    "                                  'H_FG3_PCT', \n",
    "                                  'H_FT_PCT',\n",
    "                                  'H_REB', \n",
    "                                  'H_AST', \n",
    "                                  'H_TOV', \n",
    "                                  'H_STL',\n",
    "                                  'H_BLK', \n",
    "                                  'H_PLUS_MINUS', \n",
    "                                  'H_OFF_RATING', \n",
    "                                  'H_DEF_RATING', \n",
    "                                  'H_TS_PCT', \n",
    "                                  'A_Score',\n",
    "                                  'A_W_PCT', \n",
    "                                  'A_FG_PCT', \n",
    "                                  'A_FG3_PCT',\n",
    "                                  'A_FT_PCT', \n",
    "                                  'A_REB', \n",
    "                                  'A_AST', \n",
    "                                  'A_TOV', \n",
    "                                  'A_STL',\n",
    "                                  'A_BLK', \n",
    "                                  'A_PLUS_MINUS', \n",
    "                                  'A_OFF_RATING', \n",
    "                                  'A_DEF_RATING', \n",
    "                                  'A_TS_PCT', \n",
    "                                  'Result',\n",
    "                                  'Date'])\n",
    "\n",
    "    print(games)\n",
    "    return games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function extracts NBA game data for a specied time period\n",
    "def main():\n",
    "    \n",
    "    attempts = 10\n",
    "\n",
    "    for i in range(attempts):\n",
    "        try:\n",
    "            # Plug in date ranges for game data you'd like to webscrape\n",
    "            all_games = training_set(start_year=2016, \n",
    "                                     start_month=3, \n",
    "                                     start_day=20, \n",
    "                                     end_year=2016, \n",
    "                                     end_month=4, \n",
    "                                     end_day=14,\n",
    "                                     season='2015-16', \n",
    "                                     season_start='10/27/2015')\n",
    "            \n",
    "            # Aligns data with correct columns in new DataFrame\n",
    "            df = make_dataframe(all_games)\n",
    "\n",
    "            print(df)\n",
    "            \n",
    "            # Convert game data to a csv file\n",
    "            df.to_csv(r'C:\\Users\\alvaro\\OneDrive\\Documents\\School\\Flatiron\\Projects\\NBA_Prediction_Model\\data\\nba_df_2015_v7.csv', \n",
    "                      index=False)\n",
    "        \n",
    "        except requests.exceptions.ReadTimeout:\n",
    "            \n",
    "            if i < attempts - 1:\n",
    "                continue\n",
    "            \n",
    "            else:\n",
    "                raise\n",
    "        \n",
    "        break\n",
    "\n",
    "        \n",
    "# Initialize webscraping of game data \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasons\n",
    "After webscrapping for a few days we've amassed 7 seasons of data. From the 2015-16 season to the most recent 2021-22 season. Some of the webscrapping had to be done in parts because the NBA API would error out. Doing it like this was slower but we can avoid errors and the annoyance of losing the data we've collected during the webscrape. Slow and steady wins the race.\n",
    "## 2015 - 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 222 226 209 17 218 194 86\n"
     ]
    }
   ],
   "source": [
    "# Read csv, convert date time, assign new column 'Season' with the correct year\n",
    "df_2015 = pd.read_csv('./data/nba_df_2015.csv')\n",
    "df_2015['Date'] = pd.to_datetime(df_2015['Date'])\n",
    "df_2015['Season'] = '2015-16'\n",
    "\n",
    "df_2015_2 = pd.read_csv('./data/nba_df_2015_v2.csv')\n",
    "df_2015_2['Date'] = pd.to_datetime(df_2015_2['Date'])\n",
    "df_2015_2['Season'] = '2015-16'\n",
    "\n",
    "df_2015_3 = pd.read_csv('./data/nba_df_2015_v3.csv')\n",
    "df_2015_3['Date'] = pd.to_datetime(df_2015_3['Date'])\n",
    "df_2015_3['Season'] = '2015-16'\n",
    "\n",
    "df_2015_4 = pd.read_csv('./data/nba_df_2015_v4.csv')\n",
    "df_2015_4['Date'] = pd.to_datetime(df_2015_4['Date'])\n",
    "df_2015_4['Season'] = '2015-16'\n",
    "\n",
    "df_2015_5 = pd.read_csv('./data/nba_df_2015_v5.csv')\n",
    "df_2015_5['Date'] = pd.to_datetime(df_2015_5['Date'])\n",
    "df_2015_5['Season'] = '2015-16'\n",
    "\n",
    "df_2015_6 = pd.read_csv('./data/nba_df_2015_v6.csv')\n",
    "df_2015_6['Date'] = pd.to_datetime(df_2015_6['Date'])\n",
    "df_2015_6['Season'] = '2015-16'\n",
    "\n",
    "df_2015_7 = pd.read_csv('./data/nba_df_2015_v7.csv')\n",
    "df_2015_7['Date'] = pd.to_datetime(df_2015_7['Date'])\n",
    "df_2015_7['Season'] = '2015-16'\n",
    "\n",
    "df_2015_playoffs = pd.read_csv('./data/nba_df_2015_playoffs.csv')\n",
    "df_2015_playoffs['Date'] = pd.to_datetime(df_2015_playoffs['Date'])\n",
    "df_2015_playoffs['Season'] = '2015-16'\n",
    "\n",
    "\n",
    "print(len(df_2015), len(df_2015_2), len(df_2015_3), len(df_2015_4), len(df_2015_5), len(df_2015_6), len(df_2015_7), len(df_2015_playoffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1284"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concat all DataFrames to make one for the season\n",
    "frames = [df_2015, df_2015_2, df_2015_3, df_2015_4, df_2015_5, df_2015_6, df_2015_7, df_2015_playoffs]\n",
    "df_2015_final = pd.concat(frames)\n",
    "len(df_2015_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2016 - 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122 67 161 155 227 53 164 259 79\n"
     ]
    }
   ],
   "source": [
    "# Read csv, convert date time, assign new column 'Season' with the correct year\n",
    "df_2016 = pd.read_csv('./data/nba_df_2016.csv')\n",
    "df_2016['Date'] = pd.to_datetime(df_2016['Date'])\n",
    "df_2016['Season'] = '2016-17'\n",
    "\n",
    "df_2016_2 = pd.read_csv('./data/nba_df_2016_v2.csv')\n",
    "df_2016_2['Date'] = pd.to_datetime(df_2016_2['Date'])\n",
    "df_2016_2['Season'] = '2016-17'\n",
    "\n",
    "df_2016_3 = pd.read_csv('./data/nba_df_2016_v3.csv')\n",
    "df_2016_3['Date'] = pd.to_datetime(df_2016_3['Date'])\n",
    "df_2016_3['Season'] = '2016-17'\n",
    "\n",
    "df_2016_4 = pd.read_csv('./data/nba_df_2016_v4.csv')\n",
    "df_2016_4['Date'] = pd.to_datetime(df_2016_4['Date'])\n",
    "df_2016_4['Season'] = '2016-17'\n",
    "\n",
    "df_2016_5 = pd.read_csv('./data/nba_df_2016_v5.csv')\n",
    "df_2016_5['Date'] = pd.to_datetime(df_2016_5['Date'])\n",
    "df_2016_5['Season'] = '2016-17'\n",
    "\n",
    "df_2016_6 = pd.read_csv('./data/nba_df_2016_v6.csv')\n",
    "df_2016_6['Date'] = pd.to_datetime(df_2016_6['Date'])\n",
    "df_2016_6['Season'] = '2016-17'\n",
    "\n",
    "df_2016_7 = pd.read_csv('./data/nba_df_2016_v7.csv')\n",
    "df_2016_7['Date'] = pd.to_datetime(df_2016_7['Date'])\n",
    "df_2016_7['Season'] = '2016-17'\n",
    "\n",
    "df_2016_8 = pd.read_csv('./data/nba_df_2016_v8.csv')\n",
    "df_2016_8['Date'] = pd.to_datetime(df_2016_8['Date'])\n",
    "df_2016_8['Season'] = '2016-17'\n",
    "\n",
    "df_2016_playoffs = pd.read_csv('./data/nba_df_2016_playoffs.csv')\n",
    "df_2016_playoffs['Date'] = pd.to_datetime(df_2016_playoffs['Date'])\n",
    "df_2016_playoffs['Season'] = '2016-17'\n",
    "\n",
    "\n",
    "\n",
    "print(len(df_2016), len(df_2016_2), len(df_2016_3), len(df_2016_4), len(df_2016_5), len(df_2016_6), len(df_2016_7), len(df_2016_8), len(df_2016_playoffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1287"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concat all DataFrames to make one for the season\n",
    "frames = [df_2016, df_2016_2, df_2016_3, df_2016_4, df_2016_5, df_2016_6, df_2016_7, df_2016_8, df_2016_playoffs]\n",
    "df_2016_final = pd.concat(frames)\n",
    "len(df_2016_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2017 - 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171 546 493 82\n"
     ]
    }
   ],
   "source": [
    "# Read csv, convert date time, assign new column 'Season' with the correct year\n",
    "df_2017 = pd.read_csv('./data/nba_df_2017.csv')\n",
    "df_2017['Date'] = pd.to_datetime(df_2017['Date'])\n",
    "df_2017['Season'] = '2017-18'\n",
    "\n",
    "df_2017_2 = pd.read_csv('./data/nba_df_2017_v2.csv')\n",
    "df_2017_2['Date'] = pd.to_datetime(df_2017_2['Date'])\n",
    "df_2017_2['Season'] = '2017-18'\n",
    "\n",
    "df_2017_3 = pd.read_csv('./data/nba_df_2017_v3.csv')\n",
    "df_2017_3['Date'] = pd.to_datetime(df_2017_3['Date'])\n",
    "df_2017_3['Season'] = '2017-18'\n",
    "\n",
    "df_2017_playoffs = pd.read_csv('./data/nba_df_2017_playoffs.csv')\n",
    "df_2017_playoffs['Date'] = pd.to_datetime(df_2017_playoffs['Date'])\n",
    "df_2017_playoffs['Season'] = '2017-18'\n",
    "\n",
    "\n",
    "print(len(df_2017), len(df_2017_2), len(df_2017_3), len(df_2017_playoffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1292"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concat all DataFrames to make one for the season\n",
    "frames = [df_2017, df_2017_2, df_2017_3, df_2017_playoffs]\n",
    "df_2017_final = pd.concat(frames)\n",
    "len(df_2017_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2018 - 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1203 82\n"
     ]
    }
   ],
   "source": [
    "# Read csv, convert date time, assign new column 'Season' with the correct year\n",
    "df_2018 = pd.read_csv('./data/nba_df_2018.csv')\n",
    "df_2018['Date'] = pd.to_datetime(df_2018['Date'])\n",
    "df_2018['Season'] = '2018-19'\n",
    "\n",
    "df_2018_playoffs = pd.read_csv('./data/nba_df_2018_playoffs.csv')\n",
    "df_2018_playoffs['Date'] = pd.to_datetime(df_2018_playoffs['Date'])\n",
    "df_2018_playoffs['Season'] = '2018-19'\n",
    "\n",
    "\n",
    "print(len(df_2018), len(df_2018_playoffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1285"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concat all DataFrames to make one for the season\n",
    "frames = [df_2018, df_2018_playoffs]\n",
    "df_2018_final = pd.concat(frames)\n",
    "len(df_2018_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2019 - 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "951 84 83\n"
     ]
    }
   ],
   "source": [
    "# Read csv, convert date time, assign new column 'Season' with the correct year\n",
    "df_2019 = pd.read_csv('./data/nba_df_2019.csv')\n",
    "df_2019['Date'] = pd.to_datetime(df_2019['Date'])\n",
    "df_2019['Season'] = '2019-20'\n",
    "\n",
    "df_2019_2 = pd.read_csv('./data/nba_df_2019_2.csv')\n",
    "df_2019_2['Date'] = pd.to_datetime(df_2019_2['Date'])\n",
    "df_2019_2['Season'] = '2019-20'\n",
    "\n",
    "df_2019_playoffs = pd.read_csv('./data/nba_df_2019_playoffs.csv')\n",
    "df_2019_playoffs['Date'] = pd.to_datetime(df_2019_playoffs['Date'])\n",
    "df_2019_playoffs['Season'] = '2019-20'\n",
    "\n",
    "\n",
    "print(len(df_2019), len(df_2019_2), len(df_2019_playoffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1118"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concat all DataFrames to make one for the season\n",
    "frames = [df_2019, df_2019_2, df_2019_playoffs]\n",
    "df_2019_final = pd.concat(frames)\n",
    "len(df_2019_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2020 - 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 582 469 85\n"
     ]
    }
   ],
   "source": [
    "# Read csv, convert date time, assign new column 'Season' with the correct year\n",
    "df_2021 = pd.read_csv('./data/nba_df_2020_v0.csv')\n",
    "df_2021['Date'] = pd.to_datetime(df_2021['Date'])\n",
    "df_2021['Season'] = '2020-21'\n",
    "\n",
    "df_2021_2 = pd.read_csv('./data/nba_df_2020.csv')\n",
    "df_2021_2['Date'] = pd.to_datetime(df_2021_2['Date'])\n",
    "df_2021_2['Season'] = '2020-21'\n",
    "\n",
    "df_2021_3 = pd.read_csv('./data/nba_df_2020_v2.csv')\n",
    "df_2021_3['Date'] = pd.to_datetime(df_2021_3['Date'])\n",
    "df_2021_3['Season'] = '2020-21'\n",
    "\n",
    "df_2021_playoffs = pd.read_csv('./data/nba_df_2020_playoffs.csv')\n",
    "df_2021_playoffs['Date'] = pd.to_datetime(df_2021_playoffs['Date'])\n",
    "df_2021_playoffs['Season'] = '2020-21'\n",
    "\n",
    "\n",
    "print(len(df_2021), len(df_2021_2), len(df_2021_3), len(df_2021_playoffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1141"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concat all DataFrames to make one for the season\n",
    "frames = [df_2021, df_2021_2, df_2021_3, df_2021_playoffs]\n",
    "df_2021_final = pd.concat(frames)\n",
    "len(df_2021_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2021 - 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 159 784 199 87\n"
     ]
    }
   ],
   "source": [
    "# Read csv, convert date time, assign new column 'Season' with the correct year\n",
    "df_2022 = pd.read_csv('./data/nba_game_2022.csv')\n",
    "df_2022['Date'] = pd.to_datetime(df_2022['Date'])\n",
    "df_2022['Season'] = '2021-22'\n",
    "\n",
    "df_2022_1 = pd.read_csv('./data/nba_game_2022_v1.csv')\n",
    "df_2022_1['Date'] = pd.to_datetime(df_2022_1['Date'])\n",
    "df_2022_1['Season'] = '2021-22'\n",
    "\n",
    "df_2022_2 = pd.read_csv('./data/nba_game_2022_v2.csv')\n",
    "df_2022_2['Date'] = pd.to_datetime(df_2022_2['Date'])\n",
    "df_2022_2['Season'] = '2021-22'\n",
    "\n",
    "df_2022_3 = pd.read_csv('./data/nba_game_2022_v3.csv')\n",
    "df_2022_3['Date'] = pd.to_datetime(df_2022_3['Date'])\n",
    "df_2022_3['Season'] = '2021-22'\n",
    "\n",
    "df_2022_playoffs = pd.read_csv('./data/nba_df_2022_playoffs.csv')\n",
    "df_2022_playoffs['Date'] = pd.to_datetime(df_2022_playoffs['Date'])\n",
    "df_2022_playoffs['Season'] = '2021-22'\n",
    "\n",
    "\n",
    "print(len(df_2022), len(df_2022_1), len(df_2022_2), len(df_2022_3), len(df_2022_playoffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of 2022 data: 1290\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Concat all DataFrames to make one for the season\n",
    "frames = [df_2022, df_2022_1, df_2022_2, df_2022_3, df_2022_playoffs]\n",
    "df_2022_final = pd.concat(frames)\n",
    "\n",
    "print(f\"Length of 2022 data: {len(df_2022_final)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All NBA Seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape: (8697, 34)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       "                         Home                   Away   Game_ID  H_Score  \\\n",
       " 0                LA Clippers       Sacramento Kings  21500038      114   \n",
       " 1     Portland Trail Blazers           Phoenix Suns  21500037       90   \n",
       " 2             Indiana Pacers              Utah Jazz  21500033       76   \n",
       " 3         Washington Wizards        New York Knicks  21500034      110   \n",
       " 4          Memphis Grizzlies          Brooklyn Nets  21500036      101   \n",
       " ...                      ...                    ...       ...      ...   \n",
       " 8692   Golden State Warriors         Boston Celtics  42100402      107   \n",
       " 8693          Boston Celtics  Golden State Warriors  42100403      116   \n",
       " 8694          Boston Celtics  Golden State Warriors  42100404       97   \n",
       " 8695   Golden State Warriors         Boston Celtics  42100405      104   \n",
       " 8696          Boston Celtics  Golden State Warriors  42100406       90   \n",
       " \n",
       "       H_W_PCT  H_FG_PCT  H_FG3_PCT  H_FT_PCT  H_REB  H_AST  ...  A_TOV  A_STL  \\\n",
       " 0       1.000     0.479      0.280     0.641   46.8   20.2  ...   15.0    6.6   \n",
       " 1       0.500     0.448      0.375     0.818   45.1   18.9  ...   18.1    7.6   \n",
       " 2       0.000     0.421      0.396     0.782   39.2   23.1  ...   11.0    7.7   \n",
       " 3       1.000     0.447      0.388     0.739   43.2   21.1  ...   16.0   10.5   \n",
       " 4       0.500     0.427      0.281     0.765   42.5   21.2  ...   14.4    8.7   \n",
       " ...       ...       ...        ...       ...    ...    ...  ...    ...    ...   \n",
       " 8692    0.646     0.469      0.364     0.769   45.9   27.4  ...   13.9    7.3   \n",
       " 8693    0.622     0.466      0.356     0.816   46.8   25.2  ...   15.0    8.9   \n",
       " 8694    0.622     0.466      0.356     0.816   46.8   25.2  ...   15.0    8.9   \n",
       " 8695    0.646     0.469      0.364     0.769   45.9   27.4  ...   13.9    7.3   \n",
       " 8696    0.622     0.466      0.356     0.816   46.8   25.2  ...   15.0    8.9   \n",
       " \n",
       "       A_BLK  A_PLUS_MINUS  A_OFF_RATING  A_DEF_RATING  A_TS_PCT  Result  \\\n",
       " 0       4.7           5.2         110.8         105.6     0.546       1   \n",
       " 1       4.8           1.0          97.6          97.1     0.505       0   \n",
       " 2       7.7          12.6         102.2          89.1     0.508       0   \n",
       " 3       5.5           7.0         111.5         104.0     0.529       0   \n",
       " 4       5.6         -21.5          89.7         110.2     0.463       1   \n",
       " ...     ...           ...           ...           ...       ...     ...   \n",
       " 8692    5.9           7.4         113.6         106.2     0.578       1   \n",
       " 8693    4.6           5.6         112.1         106.6     0.582       1   \n",
       " 8694    4.6           5.6         112.1         106.6     0.582       0   \n",
       " 8695    5.9           7.4         113.6         106.2     0.578       1   \n",
       " 8696    4.6           5.6         112.1         106.6     0.582       0   \n",
       " \n",
       "            Date   Season  \n",
       " 0    2015-10-31  2015-16  \n",
       " 1    2015-10-31  2015-16  \n",
       " 2    2015-10-31  2015-16  \n",
       " 3    2015-10-31  2015-16  \n",
       " 4    2015-10-31  2015-16  \n",
       " ...         ...      ...  \n",
       " 8692 2022-06-05  2021-22  \n",
       " 8693 2022-06-08  2021-22  \n",
       " 8694 2022-06-10  2021-22  \n",
       " 8695 2022-06-13  2021-22  \n",
       " 8696 2022-06-16  2021-22  \n",
       " \n",
       " [8697 rows x 34 columns])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concat all the season DataFrames into one DataFrame for processing \n",
    "frames = [df_2015_final, df_2016_final, df_2017_final, df_2018_final, df_2019_final, df_2021_final, df_2022_final]\n",
    "df = pd.concat(frames)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Shape it up\n",
    "print(f\"df shape: {df.shape}\\n\"), df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Home            0\n",
       "Away            0\n",
       "Game_ID         0\n",
       "H_Score         0\n",
       "H_W_PCT         0\n",
       "H_FG_PCT        0\n",
       "H_FG3_PCT       0\n",
       "H_FT_PCT        0\n",
       "H_REB           0\n",
       "H_AST           0\n",
       "H_TOV           0\n",
       "H_STL           0\n",
       "H_BLK           0\n",
       "H_PLUS_MINUS    0\n",
       "H_OFF_RATING    0\n",
       "H_DEF_RATING    0\n",
       "H_TS_PCT        0\n",
       "A_Score         0\n",
       "A_W_PCT         0\n",
       "A_FG_PCT        0\n",
       "A_FG3_PCT       0\n",
       "A_FT_PCT        0\n",
       "A_REB           0\n",
       "A_AST           0\n",
       "A_TOV           0\n",
       "A_STL           0\n",
       "A_BLK           0\n",
       "A_PLUS_MINUS    0\n",
       "A_OFF_RATING    0\n",
       "A_DEF_RATING    0\n",
       "A_TS_PCT        0\n",
       "Result          0\n",
       "Date            0\n",
       "Season          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No nulls. Nice and clean.\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Home</th>\n",
       "      <th>Away</th>\n",
       "      <th>Game_ID</th>\n",
       "      <th>H_Score</th>\n",
       "      <th>H_W_PCT</th>\n",
       "      <th>H_FG_PCT</th>\n",
       "      <th>H_FG3_PCT</th>\n",
       "      <th>H_FT_PCT</th>\n",
       "      <th>H_REB</th>\n",
       "      <th>H_AST</th>\n",
       "      <th>...</th>\n",
       "      <th>A_TOV</th>\n",
       "      <th>A_STL</th>\n",
       "      <th>A_BLK</th>\n",
       "      <th>A_PLUS_MINUS</th>\n",
       "      <th>A_OFF_RATING</th>\n",
       "      <th>A_DEF_RATING</th>\n",
       "      <th>A_TS_PCT</th>\n",
       "      <th>Result</th>\n",
       "      <th>Date</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Home, Away, Game_ID, H_Score, H_W_PCT, H_FG_PCT, H_FG3_PCT, H_FT_PCT, H_REB, H_AST, H_TOV, H_STL, H_BLK, H_PLUS_MINUS, H_OFF_RATING, H_DEF_RATING, H_TS_PCT, A_Score, A_W_PCT, A_FG_PCT, A_FG3_PCT, A_FT_PCT, A_REB, A_AST, A_TOV, A_STL, A_BLK, A_PLUS_MINUS, A_OFF_RATING, A_DEF_RATING, A_TS_PCT, Result, Date, Season]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 34 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No duplicates. All clear.\n",
    "duplicates = df[df.duplicated()]\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last N Games Win %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Home  Result\n",
      "8682  Golden State Warriors       1\n",
      "8688  Golden State Warriors       1\n",
      "8691  Golden State Warriors       0\n",
      "8692  Golden State Warriors       1\n",
      "8695  Golden State Warriors       1\n"
     ]
    }
   ],
   "source": [
    "prev_game_df = df[df['Date'] < '12/25/2022'][(df['Home'] == \"Golden State Warriors\") | (df['Away'] == 'Golden State Warriors')].sort_values(by = 'Date').tail(10)\n",
    "prev_game_df\n",
    "h_df = prev_game_df.iloc[:, range(0, 32, 31)]\n",
    "\n",
    "h_df = h_df.loc[h_df['Home'] == 'Golden State Warriors'] \n",
    "print(h_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_avg_win_pct_last_n_games(team, game_date, df, n):\n",
    "    prev_game_df = df[df['Date'] < game_date][(df['Home'] == team) | (df['Away'] == team)].sort_values(by = 'Date').tail(n)\n",
    "    \n",
    "    wins = 0 \n",
    "    \n",
    "    result_df = prev_game_df.iloc[:, range(0, 32, 31)]\n",
    "    h_df = result_df.loc[result_df['Home'] == team] \n",
    "    \n",
    "    h_wins = h_df.loc[h_df['Result'] == 1]\n",
    "    \n",
    "    wins += len(h_wins)\n",
    "      \n",
    "    a_df = result_df.loc[result_df['Home'] != team]\n",
    "    a_wins = a_df.loc[a_df['Result'] == 0]\n",
    "    \n",
    "    wins += len(a_wins)\n",
    "\n",
    "    return wins/n\n",
    "\n",
    "get_avg_win_pct_last_n_games('Golden State Warriors', '12/25/2022', df, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\linecache.py:72\u001b[0m, in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     stat \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\alvaro\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_4880\\\\991417696.py'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m a_team \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAway\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m df\u001b[38;5;241m.\u001b[39mloc[index,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHome_W_Pct_10\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m get_avg_win_pct_last_n_games(h_team, game_date, df, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m df\u001b[38;5;241m.\u001b[39mloc[index,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAway_W_Pct_10\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mget_avg_win_pct_last_n_games\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_team\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgame_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36mget_avg_win_pct_last_n_games\u001b[1;34m(team, game_date, df, n)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_avg_win_pct_last_n_games\u001b[39m(team, game_date, df, n):\n\u001b[1;32m----> 2\u001b[0m     prev_game_df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgame_date\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHome\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mteam\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAway\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mteam\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msort_values(by \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtail(n)\n\u001b[0;32m      4\u001b[0m     wins \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n\u001b[0;32m      6\u001b[0m     result_df \u001b[38;5;241m=\u001b[39m prev_game_df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m31\u001b[39m)]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3496\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3494\u001b[0m \u001b[38;5;66;03m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[0;32m   3495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n\u001b[1;32m-> 3496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_bool_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3498\u001b[0m \u001b[38;5;66;03m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[0;32m   3499\u001b[0m \u001b[38;5;66;03m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n\u001b[0;32m   3500\u001b[0m is_single_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(key)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3540\u001b[0m, in \u001b[0;36mDataFrame._getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3530\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_getitem_bool_array\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m   3531\u001b[0m     \u001b[38;5;66;03m# also raises Exception if object array with NA values\u001b[39;00m\n\u001b[0;32m   3532\u001b[0m     \u001b[38;5;66;03m# warning here just in case -- previously __setitem__ was\u001b[39;00m\n\u001b[0;32m   3533\u001b[0m     \u001b[38;5;66;03m# reindexing but __getitem__ was not; it seems more reasonable to\u001b[39;00m\n\u001b[0;32m   3534\u001b[0m     \u001b[38;5;66;03m# go with the __setitem__ behavior since that is more consistent\u001b[39;00m\n\u001b[0;32m   3535\u001b[0m     \u001b[38;5;66;03m# with all other indexing behavior\u001b[39;00m\n\u001b[0;32m   3536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Series) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m key\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex):\n\u001b[0;32m   3537\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   3538\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBoolean Series key will be reindexed to match DataFrame index.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3539\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m-> 3540\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[43mfind_stack_level\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   3541\u001b[0m         )\n\u001b[0;32m   3542\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex):\n\u001b[0;32m   3543\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3544\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mItem wrong length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(key)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3545\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_exceptions.py:32\u001b[0m, in \u001b[0;36mfind_stack_level\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_stack_level\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    Find the first place in the stack that is not inside pandas\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;124;03m    (tests notwithstanding).\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     stack \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     pkg_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(pd\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\inspect.py:1554\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(context)\u001b[0m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstack\u001b[39m(context\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;124;03m\"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetouterframes\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getframe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\inspect.py:1531\u001b[0m, in \u001b[0;36mgetouterframes\u001b[1;34m(frame, context)\u001b[0m\n\u001b[0;32m   1529\u001b[0m framelist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1530\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m frame:\n\u001b[1;32m-> 1531\u001b[0m     frameinfo \u001b[38;5;241m=\u001b[39m (frame,) \u001b[38;5;241m+\u001b[39m \u001b[43mgetframeinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1532\u001b[0m     framelist\u001b[38;5;241m.\u001b[39mappend(FrameInfo(\u001b[38;5;241m*\u001b[39mframeinfo))\n\u001b[0;32m   1533\u001b[0m     frame \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mf_back\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\inspect.py:1505\u001b[0m, in \u001b[0;36mgetframeinfo\u001b[1;34m(frame, context)\u001b[0m\n\u001b[0;32m   1503\u001b[0m start \u001b[38;5;241m=\u001b[39m lineno \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m context\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1505\u001b[0m     lines, lnum \u001b[38;5;241m=\u001b[39m \u001b[43mfindsource\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   1507\u001b[0m     lines \u001b[38;5;241m=\u001b[39m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\inspect.py:820\u001b[0m, in \u001b[0;36mfindsource\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    817\u001b[0m file \u001b[38;5;241m=\u001b[39m getsourcefile(\u001b[38;5;28mobject\u001b[39m)\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file:\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;66;03m# Invalidate cache if needed.\u001b[39;00m\n\u001b[1;32m--> 820\u001b[0m     \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    822\u001b[0m     file \u001b[38;5;241m=\u001b[39m getfile(\u001b[38;5;28mobject\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\compilerop.py:193\u001b[0m, in \u001b[0;36mcheck_linecache_ipython\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"Call linecache.checkcache() safely protecting our cached values.\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# First call the original checkcache as intended\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_checkcache_ori\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# Then, update back the cache with our data, so that tracebacks related\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# to our compiled codes can be produced.\u001b[39;00m\n\u001b[0;32m    196\u001b[0m linecache\u001b[38;5;241m.\u001b[39mcache\u001b[38;5;241m.\u001b[39mupdate(linecache\u001b[38;5;241m.\u001b[39m_ipython_cache)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\linecache.py:72\u001b[0m, in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m   \u001b[38;5;66;03m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     stat \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     cache\u001b[38;5;241m.\u001b[39mpop(filename, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for season in df['Season'].unique() :\n",
    "    \n",
    "    season_stats = df[df['Season'] == season].sort_values(by='Date').reset_index(drop=True)\n",
    "    \n",
    "    for index, row in df.iterrows() : \n",
    "        game_id = row['Game_ID']\n",
    "        game_date = row['Date']\n",
    "        h_team = row['Home']\n",
    "        a_team = row['Away']\n",
    "        \n",
    "        df.loc[index,'Home_W_Pct_10'] = get_avg_win_pct_last_n_games(h_team, game_date, df, 10)\n",
    "        \n",
    "        df.loc[index,'Away_W_Pct_10'] = get_avg_win_pct_last_n_games(a_team, game_date, df, 10)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['Season'] == '2021-22'].tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elo Rating\n",
    "For those new to Elo, here are its essential features:\n",
    "\n",
    "1. Elo ratings depend on the final score of a game and where it was played (home-court advantage). Elo includes both regular-season and playoff games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Teams gain Elo points after winning and lose points after losing. A team gains more points for an upset win and for winning by wide margins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Elo system is zero-sum. When the Golden State Warriors were on their historic run during the 2015-16 season they had a record of 73-9 and the 2nd highest Elo rating in NBA history at 1839. During those NBA Finals the Warriors started with a 3-1 series lead against the Cleveland Cavaliers. In the end, the Cavaliers pulled off a huge upset and actually won the championship, the Warriors never passed the 1995-96 Chicago Bulls 1853 Elo peak, and their Elo had dropped to 1756."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Ratings are tied to game-by-game rather than a season-by-season performance. So we can see changes in a team’s “form” over the course of the season.\n",
    "\n",
    "The long-term average Elo rating is 1500, although it may vary in any particular year depending on how recently the league has expanded. Over 90% of team ratings are between 1300 (awful) and 1700 (great), but historically terrible or great teams may fall outside that range.\n",
    "\n",
    "An optimal K for the NBA is 20. This is higher than we expected; it’s in the same range as the K used for NFL and international soccer Elo ratings even though the NBA plays far more games than those sports. It’s much higher than the optimal K for baseball. It implies that you ought to give relatively high weight to an NBA team’s recent performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each team begins with a 1500\n",
    "$$R_{i+1} = k * (S_{team} - E_{team} + R_{i})$$\n",
    "- S equals 1 if the team wins and 0 if they lose\n",
    "- E is the expected win probability of the team "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E_{team} = \\frac{1}{1+10^{\\frac{opp\\_elo - team\\_elo}{400}}}$$\n",
    "- Elo’s K-factor determines how quickly the rating reacts to new game results. It's set to efficiently account for new data but not overreact to it. We want to minimize autocorrelation. If K is set too high, the ratings bounce around too much; if it’s set too low, Elo won't recognize important changes in team quality quickly enough. In short, k is a moving constant that depends on margin of victory and difference in Elo ratings. An optimal K for the NBA is 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$k = 20\\frac{(MOV_{winner} + 3)^{0.8}}{7.5 + 0.006(elo\\_diff _{winner})} $$\n",
    "- Team year-to-year carry-over. Rather than resetting each team’s Elo when a new season starts, Elo carries over some of a team’s rating from one season to the next. In NBA Elo ratings, teams retain three-quarters of their Elo from the end of the previous season."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$(R * 0.75) + (0.25 * 1505)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Home and road team win probabilities implied by Elo ratings and home court adjustment \n",
    "def win_probs(home_elo, away_elo, home_court_advantage) :\n",
    "    h = math.pow(10, home_elo/400)\n",
    "    r = math.pow(10, away_elo/400)\n",
    "    a = math.pow(10, home_court_advantage/400) \n",
    "\n",
    "    denom = r + a * h\n",
    "    home_prob = a * h / denom\n",
    "    away_prob = r / denom \n",
    "  \n",
    "    return home_prob, away_prob\n",
    "\n",
    "# Odds the home team will win based on elo ratings and home court advantage\n",
    "def home_odds_on(home_elo, away_elo, home_court_advantage) :\n",
    "    h = math.pow(10, home_elo / 400)\n",
    "    r = math.pow(10, away_elo / 400)\n",
    "    a = math.pow(10, home_court_advantage / 400)\n",
    "    return a * h / r\n",
    "\n",
    "# Function determines the constant used in the elo rating, based on margin of victory and difference in elo ratings\n",
    "def elo_k(MOV, elo_diff):\n",
    "    k = 20 # Optimal K is 20\n",
    "    if MOV > 0:\n",
    "        multiplier = (MOV + 3) ** (0.8) / (7.5 + 0.006 * (elo_diff))\n",
    "    else:\n",
    "        multiplier = (-MOV + 3) ** (0.8) / (7.5 + 0.006 * (-elo_diff))\n",
    "    return k*multiplier\n",
    "\n",
    "\n",
    "# Updates the home and away teams elo ratings after a game \n",
    "def update_elo(home_score, away_score, home_elo, away_elo, home_court_advantage) :\n",
    "    home_prob, away_prob = win_probs(home_elo, away_elo, home_court_advantage) \n",
    "\n",
    "    if (home_score - away_score > 0) :\n",
    "        home_win = 1 \n",
    "        away_win = 0 \n",
    "    else :\n",
    "        home_win = 0 \n",
    "        away_win = 1 \n",
    "  \n",
    "    k = elo_k(home_score - away_score, home_elo - away_elo)\n",
    "\n",
    "    updated_home_elo = home_elo + k * (home_win - home_prob) \n",
    "    updated_away_elo = away_elo + k * (away_win - away_prob)\n",
    "    \n",
    "    return updated_home_elo, updated_away_elo\n",
    "\n",
    "\n",
    "# Takes into account prev season elo\n",
    "# The reason we revert to a mean of 1505 rather than 1500 is that \n",
    "# there are liable to be a couple of relatively recent expansion teams in the league at any given time\n",
    "def get_prev_elo(team, date, season, team_stats, elo_df) :\n",
    "    prev_game = team_stats[team_stats['Date'] < game_date][(team_stats['Home'] == team) | (team_stats['Away'] == team)].sort_values(by = 'Date').tail(1).iloc[0] \n",
    "\n",
    "    if team == prev_game['Home'] :\n",
    "        elo_rating = elo_df[elo_df['Game_ID'] == prev_game['Game_ID']]['H_Team_Elo_After'].values[0]\n",
    "    else :\n",
    "        elo_rating = elo_df[elo_df['Game_ID'] == prev_game['Game_ID']]['A_Team_Elo_After'].values[0]\n",
    "  \n",
    "    if prev_game['Season'] != season :\n",
    "        return (0.75 * elo_rating) + (0.25 * 1505) # Year-to-Year Carry-Over\n",
    "    else :\n",
    "        return elo_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by ='Date', \n",
    "               inplace=True)\n",
    "\n",
    "df.reset_index(inplace=True, \n",
    "               drop = True)\n",
    "\n",
    "elo_df = pd.DataFrame(columns=['Game_ID', \n",
    "                               'H_Team', \n",
    "                               'A_Team', \n",
    "                               'H_Team_Elo_Before', \n",
    "                               'A_Team_Elo_Before', \n",
    "                               'H_Team_Elo_After', \n",
    "                               'A_Team_Elo_After'])\n",
    "\n",
    "teams_elo_df = pd.DataFrame(columns=['Game_ID',\n",
    "                                     'Team', \n",
    "                                     'Elo', \n",
    "                                     'Date', \n",
    "                                     'Where_Played', \n",
    "                                     'Season']) \n",
    "\n",
    "for index, row in df.iterrows(): \n",
    "    game_id = row['Game_ID']\n",
    "    game_date = row['Date']\n",
    "    season = row['Season']\n",
    "    h_team, a_team = row['Home'], row['Away']\n",
    "    h_score, a_score = row['H_Score'], row['A_Score'] \n",
    "\n",
    "    if (h_team not in elo_df['H_Team'].values and h_team not in elo_df['A_Team'].values) :\n",
    "        h_team_elo_before = 1500\n",
    "    else :\n",
    "        h_team_elo_before = get_prev_elo(h_team, game_date, season, df, elo_df)\n",
    "\n",
    "    if (a_team not in elo_df['H_Team'].values and a_team not in elo_df['A_Team'].values) :\n",
    "        a_team_elo_before = 1500\n",
    "    else :\n",
    "        a_team_elo_before = get_prev_elo(a_team, game_date, season, df, elo_df)\n",
    "\n",
    "    h_team_elo_after, a_team_elo_after = update_elo(h_score, a_score, h_team_elo_before, a_team_elo_before, 69)\n",
    "\n",
    "    new_row = {'Game_ID': game_id, 'H_Team': h_team, 'A_Team': a_team, 'H_Team_Elo_Before': h_team_elo_before, 'A_Team_Elo_Before': a_team_elo_before, \\\n",
    "                                                                        'H_Team_Elo_After' : h_team_elo_after, 'A_Team_Elo_After': a_team_elo_after}\n",
    "    teams_row_one = {'Game_ID': game_id,'Team': h_team, 'Elo': h_team_elo_before, 'Date': game_date, 'Where_Played': 'Home', 'Season': season}\n",
    "    teams_row_two = {'Game_ID': game_id,'Team': a_team, 'Elo': a_team_elo_before, 'Date': game_date, 'Where_Played': 'Away', 'Season': season}\n",
    "  \n",
    "    elo_df = elo_df.append(new_row, ignore_index = True)\n",
    "    teams_elo_df = teams_elo_df.append(teams_row_one, ignore_index=True)\n",
    "    teams_elo_df = teams_elo_df.append(teams_row_two, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#teams_elo_df.set_index([\"Team\"], append=True)\n",
    "#dataset = teams_elo_df.pivot(index=\"Team\",values=\"Elo\", columns=\"Date\")\n",
    "dates = list(set([d.strftime(\"%m-%d-%Y\") for d in teams_elo_df[\"Date\"]]))\n",
    "dates = sorted(dates, key=lambda x: time.strptime(x, '%m-%d-%Y'))\n",
    "teams = df[\"Away\"]\n",
    "dataset = pd.DataFrame(columns=dates)\n",
    "dataset[\"Team\"] = teams.drop_duplicates()\n",
    "dataset = dataset.set_index(\"Team\")\n",
    "\n",
    "for index, row in teams_elo_df.iterrows():\n",
    "    date = row[\"Date\"].strftime(\"%m-%d-%Y\")\n",
    "    team = row[\"Team\"]\n",
    "    elo = row[\"Elo\"]\n",
    "    dataset[date][team] = elo\n",
    "\n",
    "teams_elo_df['Elo'] = teams_elo_df['Elo'].astype(float)\n",
    "\n",
    "elo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(elo_df.drop(columns=['H_Team', 'A_Team']), on ='Game_ID')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'C:/Users/alvaro/OneDrive/Documents/School/Flatiron/Projects/NBA_Prediction_Model/data/nba_raw.csv', \n",
    "          index=False)\n",
    "print(f'The final dataset consists of three seasons and {len(df)} games.')\n",
    "df = df.reset_index(drop=True)\n",
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/nba_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.corr()['Result'].abs().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"darkgrid\"):\n",
    "    fig, ax = plt.subplots(figsize=(44, 34))\n",
    "    \n",
    "    mask = np.triu(np.ones_like(df.corr(), dtype=bool))\n",
    "    ax = sns.heatmap(abs(df.corr()),mask=mask,annot=True)\n",
    "    fig.savefig('images/Corelation_Heatmap');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast \"H_Team_Elo_Before\" and \"A_Team_Elo_Before\" as floats\n",
    "df[\"H_Team_Elo_Before\"] = df.H_Team_Elo_Before.astype(float)\n",
    "df[\"A_Team_Elo_Before\"] = df.A_Team_Elo_Before.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns that have data on post-game stats\n",
    "df = df.drop(['H_Team_Elo_After', 'A_Team_Elo_After', 'H_Score', 'A_Score'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelevant columns\n",
    "df = df.drop(['Home', 'Away', 'Game_ID', 'Date', 'Season'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'C:\\Users\\alvaro\\OneDrive\\Documents\\School\\Flatiron\\Projects\\NBA_Prediction_Model\\data\\nba.csv', \n",
    "          index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/nba.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(44, 34))\n",
    "correlation = df[['H_W_PCT', \n",
    "                  'H_REB', \n",
    "                  'H_AST',\n",
    "                  'H_TOV', \n",
    "                  'H_STL', \n",
    "                  'H_BLK', \n",
    "                  'H_PLUS_MINUS', \n",
    "                  'H_OFF_RATING',\n",
    "                  'H_DEF_RATING', \n",
    "                  'H_TS_PCT', \n",
    "                  'H_Team_Elo_Before', \n",
    "                  'Home_W_Pct_10', \n",
    "                  'Result'\n",
    "                  ]]\n",
    "\n",
    "sns.heatmap(correlation.corr(), annot=True);\n",
    "# correlation\n",
    "# sns.heatmap(df.corr(), annot=True);\n",
    "# sns.heatmap(df['Result'].corr(), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.corr()['Result'].abs().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"darkgrid\"):\n",
    "    fig, ax = plt.subplots(figsize=(44, 34))\n",
    "    \n",
    "    mask = np.triu(np.ones_like(df.corr(), dtype=bool))\n",
    "    ax = sns.heatmap(abs(df.corr()),mask=mask,annot=True)\n",
    "    fig.savefig('images/Corelation_Heatmap_2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = 'Result')\n",
    "\n",
    "y = df['Result']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42)\n",
    "\n",
    "print(f'X train shape: {X_train.shape}')\n",
    "print(f'X test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"Raw Counts \n",
    "{df[\"Result\"].value_counts()}\\n\n",
    "Percentages \n",
    "{df[\"Result\"].value_counts(normalize=True)}\n",
    "\n",
    "\n",
    "We would get an accuracy score of {np.round(df[\"Result\"].value_counts(normalize=True)[1], 4)} with a baseline model, i.e. about 56.6% accuracy\n",
    "\n",
    "This is because about 55.79% of the results are wins\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"Train percent wins\n",
    "{y_train.value_counts(normalize=True)}\\n\"\"\")\n",
    "\n",
    "print(f\"\"\"Test percent wins: \n",
    "{y_test.value_counts(normalize=True)}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWithCV():\n",
    "    '''Structure to save the model and more easily see its crossvalidation'''\n",
    "    \n",
    "    def __init__(self, model, model_name, X, y, cv_now=True):\n",
    "        self.model = model\n",
    "        self.name = model_name\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        # For CV results\n",
    "        self.cv_results = None\n",
    "        self.cv_mean = None\n",
    "        self.cv_median = None\n",
    "        self.cv_std = None\n",
    "        #\n",
    "        if cv_now:\n",
    "            self.cross_validate()\n",
    "        \n",
    "    def cross_validate(self, X=None, y=None, kfolds=10):\n",
    "        '''\n",
    "        Perform cross-validation and return results.\n",
    "        \n",
    "        Args: \n",
    "          X:\n",
    "            Optional; Training data to perform CV on. Otherwise use X from object\n",
    "          y:\n",
    "            Optional; Training data to perform CV on. Otherwise use y from object\n",
    "          kfolds:\n",
    "            Optional; Number of folds for CV (default is 10)  \n",
    "        '''\n",
    "        \n",
    "        cv_X = X if X else self.X\n",
    "        cv_y = y if y else self.y\n",
    "\n",
    "        self.cv_results = cross_val_score(self.model, cv_X, cv_y, cv=kfolds)\n",
    "        self.cv_mean = np.mean(self.cv_results)\n",
    "        self.cv_median = np.median(self.cv_results)\n",
    "        self.cv_std = np.std(self.cv_results)\n",
    "\n",
    "        \n",
    "    def print_cv_summary(self):\n",
    "        cv_summary = (\n",
    "        f'''CV Results for `{self.name}` model:\n",
    "            {self.cv_mean:.5f} ± {self.cv_std:.5f} accuracy\n",
    "        ''')\n",
    "        print(cv_summary)\n",
    "\n",
    "        \n",
    "    def plot_cv(self, ax):\n",
    "        '''\n",
    "        Plot the cross-validation values using the array of results and given \n",
    "        Axis for plotting.\n",
    "        '''\n",
    "        ax.set_title(f'CV Results for `{self.name}` Model')\n",
    "        # Thinner violinplot with higher bw\n",
    "        sns.violinplot(y=self.cv_results, ax=ax, bw=.4)\n",
    "        sns.swarmplot(\n",
    "                y=self.cv_results,\n",
    "                color='orange',\n",
    "                size=10,\n",
    "                alpha= 0.8,\n",
    "                ax=ax\n",
    "        )\n",
    "\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dummy/Baseliner\n",
    "\n",
    "\n",
    "\n",
    "estimator = DummyRegressor(strategy='mean')\n",
    "\n",
    "# Create Dummy/Baseliner\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('estimator', DummyRegressor(strategy='mean'))\n",
    "])\n",
    "\n",
    "cv = ModelWithCV(pipe, 'estimator', X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "\n",
    "cv.plot_cv(ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.print_cv_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('poly', PolynomialFeatures(degree=2)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=4)),\n",
    "    ('estimator', LogisticRegression(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {}\n",
    "param_grid['estimator__C'] = [100, 10, 1.0, 0.1, 0.01]\n",
    "param_grid['estimator__solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "param_grid['estimator__penalty'] = ['l2']\n",
    "# param_grid['estimator__class_weight'] = ['balanced', None]\n",
    "# param_grid['estimator__n_jobs'] = [-1]\n",
    "# param_grid['estimator__l1_ratio'] = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=pipe, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42), \n",
    "                           return_train_score=True, \n",
    "                           scoring='accuracy', \n",
    "                           n_jobs=-1,\n",
    "                           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('poly', PolynomialFeatures(degree=2)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=4)),\n",
    "    ('estimator', RidgeClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "param_grid = {}\n",
    "param_grid['estimator__alpha'] = np.linspace(0, 200, num=50)\n",
    "# param_grid['estimator__solver'] = ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']\n",
    "# param_grid['estimator__subsample'] = [0.5, 0.7, 1.0]\n",
    "# param_grid['estimator__max_depth'] = [3, 7, 9]\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipe, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42), \n",
    "                           return_train_score=True, \n",
    "                           scoring='accuracy',\n",
    "                           n_jobs=-1,\n",
    "                           verbose=2)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest - Random Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('estimator', RandomForestClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {}\n",
    "param_grid['estimator__n_estimators'] = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
    "param_grid['estimator__max_features'] = ['auto', 'sqrt', 'log2']\n",
    "param_grid['estimator__max_depth'] = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "param_grid['estimator__min_samples_split'] = [2, 5, 10]\n",
    "param_grid['estimator__min_samples_leaf'] = [1, 2, 4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = RandomizedSearchCV(estimator=pipe, \n",
    "                                 param_distributions=param_grid, \n",
    "                                 cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42), \n",
    "                                 return_train_score=True, \n",
    "                                 scoring='accuracy', \n",
    "                                 n_iter=100, \n",
    "                                 random_state=42, \n",
    "                                 n_jobs=-1, \n",
    "                                 verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest - Grid Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('estimator', RandomForestClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {}\n",
    "param_grid['estimator__n_estimators'] = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
    "param_grid['estimator__max_features'] = ['auto', 'sqrt']\n",
    "param_grid['estimator__max_depth'] = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "param_grid['estimator__min_samples_split'] = [2, 5, 10]\n",
    "param_grid['estimator__min_samples_leaf'] = [1, 2, 4]\n",
    "# param_grid['estimator__bootstrap'] = [True, False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=pipe, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42), \n",
    "                           return_train_score=True, \n",
    "                           scoring='accuracy',\n",
    "                           n_jobs=-1,\n",
    "                           verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('estimator', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "param_grid = {}\n",
    "param_grid['estimator__n_estimators'] = [10, 100, 1000]\n",
    "param_grid['estimator__max_features'] = ['sqrt', 'log2']\n",
    "# param_grid['estimator__max_depth'] = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "# param_grid['estimator__min_samples_split'] = [2, 5, 10]\n",
    "# param_grid['estimator__min_samples_leaf'] = [1, 2, 4]\n",
    "# param_grid['estimator__bootstrap'] = [True, False]\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipe, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42), \n",
    "                           return_train_score=True, \n",
    "                           scoring='accuracy',\n",
    "                           n_jobs=-1,\n",
    "                           verbose=2)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('estimator', GaussianNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {}\n",
    "param_grid['estimator__var_smoothing'] = np.logspace(0,-11, num=100)\n",
    "#param_grid['estimator__var_smoothing'] = [1e-11, 1e-10, 1e-9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = RandomizedSearchCV(estimator=pipe, \n",
    "                                 param_distributions=param_grid, \n",
    "                                 cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42), \n",
    "                                 return_train_score=True, \n",
    "                                 scoring='accuracy', \n",
    "                                 random_state=42,\n",
    "                                 n_jobs = -1,\n",
    "                                 verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('estimator', GaussianNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {}\n",
    "param_grid['estimator__var_smoothing'] = np.logspace(0,-11, num=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=pipe, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42), \n",
    "                           return_train_score=True, \n",
    "                           scoring='accuracy',\n",
    "                           n_jobs=-1,\n",
    "                           verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('estimator', KNeighborsClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {}\n",
    "param_grid['estimator__n_neighbors'] = range(1, 21, 2)\n",
    "param_grid['estimator__p'] = [1, 2]\n",
    "param_grid['estimator__weights'] = ['uniform', 'distance']\n",
    "param_grid['estimator__metric'] = ['euclidean', 'manhattan', 'minkowski']\n",
    "# param_grid['estimator__leaf_size'] = (20, 40, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=pipe, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42), \n",
    "                           return_train_score=True, \n",
    "                           scoring='accuracy',\n",
    "                           n_jobs = -1,\n",
    "                           verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('estimator', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "param_grid = {}\n",
    "param_grid['estimator__n_estimators'] = [10, 100, 1000]\n",
    "param_grid['estimator__learning_rate'] = [0.001, 0.01, 0.1]\n",
    "param_grid['estimator__subsample'] = [0.5, 0.7, 1.0]\n",
    "param_grid['estimator__max_depth'] = [3, 7, 9]\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipe, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42), \n",
    "                           return_train_score=True, \n",
    "                           scoring='accuracy',\n",
    "                           n_jobs=-1,\n",
    "                           verbose=2)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('estimator', SVC(random_state=42))\n",
    "])\n",
    "\n",
    "kernel = ['poly', 'rbf', 'sigmoid']\n",
    "C = [50, 10, 1.0, 0.1, 0.01]\n",
    "gamma = ['scale']\n",
    "\n",
    "\n",
    "param_grid = {}\n",
    "param_grid['estimator__kernel'] = ['poly', 'rbf', 'sigmoid']\n",
    "param_grid['estimator__C'] = [50, 10, 1.0, 0.1, 0.01]\n",
    "param_grid['estimator__gamma'] = ['scale']\n",
    "\n",
    "grid_search = RandomizedSearchCV(estimator=pipe, \n",
    "                                 param_distributions=param_grid, \n",
    "                                 cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42), \n",
    "                                 return_train_score=True, \n",
    "                                 scoring='accuracy', \n",
    "                                 n_iter=100, \n",
    "                                 random_state=42, \n",
    "                                 n_jobs=-1, \n",
    "                                 verbose=2)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipe, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42), \n",
    "                           return_train_score=True, \n",
    "                           scoring='accuracy',\n",
    "                           n_jobs = -1,\n",
    "                           verbose=2)\n",
    "\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('estimator', xgboost.XGBRegressor(random_state=42, objective='reg:squarederror'))\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {}\n",
    "param_grid['estimator__min_child_weight'] = [1, 5, 10],\n",
    "param_grid['estimator__gamma'] = [0.5, 1, 1.5, 2, 5],\n",
    "param_grid['estimator__subsample'] = [0.6, 0.8, 1.0],\n",
    "param_grid['estimator__colsample_bytree'] = [0.6, 0.8, 1.0],\n",
    "param_grid['estimator__max_depth'] = [3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=pipe, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=10, \n",
    "                           return_train_score=True, \n",
    "                           scoring='accuracy',\n",
    "                           n_jobs = -1,\n",
    "                           verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean training score\n",
    "grid_train_score = np.mean(grid_search.cv_results_['mean_train_score'])\n",
    "\n",
    "# Mean test score\n",
    "grid_test_score = grid_search.score(X_test, y_test)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(X_train, y_train)\n",
    "y_pred = best_grid.predict(X_test)\n",
    "\n",
    "print(f\"Mean Training Score: {grid_train_score:.2%}\\n\")\n",
    "print(f\"Mean Test Score: {grid_test_score:.2%}\\n\")\n",
    "\n",
    "print(f\"Accuracy Score: {accuracy_score(y_test, y_pred):.2%}\\n\")\n",
    "print(f\"Optimal Parameters: {grid_search.best_params_}\\n\")\n",
    "print(f\"Testing Accuracy: {grid_search.best_score_:.2%}\\n\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a confusion matrix on the test data\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(final_model, X_test, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#script to test the effectivenes of each model, uses default parameters\n",
    "#test six different classification models \n",
    "def run_exps(X_train, y_train, X_test, y_test) :\n",
    "    '''\n",
    "    Lightweight script to test many models and find winners\n",
    "    :param X_train: training split\n",
    "    :param y_train: training target vector\n",
    "    :param X_test: test split\n",
    "    :param y_test: test target vector\n",
    "    :return: DataFrame of predictions\n",
    "    '''\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    models = [\n",
    "          ('LogReg', LogisticRegression()), \n",
    "          ('RF', RandomForestClassifier()),\n",
    "          ('KNN', KNeighborsClassifier()),\n",
    "          ('SVM', SVC()), \n",
    "          ('GNB', GaussianNB()),\n",
    "          ('XGB', XGBClassifier())\n",
    "        ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    names = []\n",
    "    \n",
    "    scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted', 'roc_auc']\n",
    "    \n",
    "    target_names = ['win', 'loss']\n",
    "    \n",
    "    for name, model in models:\n",
    "        \n",
    "        kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=90210)\n",
    "        cv_results = model_selection.cross_validate(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "        clf = model.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        print(name)\n",
    "        print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "        \n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        \n",
    "        this_df = pd.DataFrame(cv_results)\n",
    "        this_df['model'] = name\n",
    "        dfs.append(this_df)\n",
    "        \n",
    "    final = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    return final\n",
    "final = run_exps(X_train, y_train, X_test, y_test)\n",
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstraps = []\n",
    "for model in list(set(final.model.values)):\n",
    "    model_df = final.loc[final.model == model]\n",
    "    bootstrap = model_df.sample(n=30, replace=True)\n",
    "    bootstraps.append(bootstrap)\n",
    "        \n",
    "bootstrap_df = pd.concat(bootstraps, ignore_index=True)\n",
    "results_long = pd.melt(bootstrap_df,id_vars=['model'],var_name='metrics', value_name='values')\n",
    "time_metrics = ['fit_time','score_time'] # fit time metrics\n",
    "## PERFORMANCE METRICS\n",
    "results_long_nofit = results_long.loc[~results_long['metrics'].isin(time_metrics)] # get df without fit data\n",
    "results_long_nofit = results_long_nofit.sort_values(by='values')\n",
    "## TIME METRICS\n",
    "results_long_fit = results_long.loc[results_long['metrics'].isin(time_metrics)] # df with fit data\n",
    "results_long_fit = results_long_fit.sort_values(by='values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(20, 12))\n",
    "sns.set(font_scale=2.5)\n",
    "g = sns.boxplot(x=\"model\", y=\"values\", hue=\"metrics\", data=results_long_nofit, palette=\"Set3\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('Comparison of Model by Classification Metric')\n",
    "plt.savefig('./benchmark_models_performance.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "sns.set(font_scale=2.5)\n",
    "g = sns.boxplot(x=\"model\", y=\"values\", hue=\"metrics\", data=results_long_fit, palette=\"Set3\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title('Comparison of Model by Fit and Score Time')\n",
    "plt.savefig('./benchmark_models_time.png',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = list(set(results_long_nofit.metrics.values))\n",
    "bootstrap_df.groupby(['model'])[metrics].agg([np.std, np.mean])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Selected Model\n",
    "- grid search for parameters \n",
    "- Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian NB only has one parameter 'var_smoothing'\n",
    "# Portion of the largest variance of all features that is added to variances for calculation stability.\n",
    "# Number of different combinations of parameters \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "nb_classifier = GaussianNB()\n",
    "\n",
    "target_names = ['Win', 'Loss']\n",
    "\n",
    "params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=90210)\n",
    "\n",
    "gs_NB = GridSearchCV(estimator=nb_classifier, \n",
    "                 param_grid=params_NB, \n",
    "                 cv=kfold,   \n",
    "                 verbose=1, \n",
    "                 scoring='accuracy', n_jobs=-1) \n",
    "\n",
    "gs_NB.fit(X_train, y_train)\n",
    "\n",
    "best_gs_grid = gs_NB.best_estimator_\n",
    "best_gs_grid.fit(X_train, y_train)\n",
    "y_pred_best_gs = best_gs_grid.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred_best_gs, target_names=target_names))\n",
    "gs_NB.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusionMatrix = confusion_matrix(y_test, y_pred_best_gs)\n",
    "print(confusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusionMatrix = confusion_matrix(y_test, y_pred_best_gs)  \n",
    "\n",
    "    # Code below prints model accuracy information\n",
    "print('Coefficient Information:')\n",
    "\n",
    "for i in range(len(featureColumns)):  \n",
    "\n",
    "    logregCoefficients = logreg.coef_\n",
    "\n",
    "    currentFeature = featureColumns[i]\n",
    "    currentCoefficient = logregCoefficients[0][i]\n",
    "\n",
    "    print(currentFeature + ': ' + str(currentCoefficient))\n",
    "\n",
    "print('----------------------------------')\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(Y_test, Y_pred))\n",
    "print(\"Precision:\", metrics.precision_score(Y_test, Y_pred))\n",
    "print(\"Recall:\", metrics.recall_score(Y_test, Y_pred))\n",
    "\n",
    "print('----------------------------------')\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(confusionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Saves the model in folder to be used in future\n",
    "# filename should be end in '.pkl'\n",
    "def save_model(model, filename):\n",
    "\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(model, file)\n",
    "save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
